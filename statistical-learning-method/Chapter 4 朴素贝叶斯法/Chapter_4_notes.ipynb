{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 4 notes.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN99TnEoU8eAsnRRY8P7kQJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vvvu/potential-chainsaw/blob/main/statistical-learning-method/Chapter%204%20%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95/Chapter_4_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKN9BcGxPhkv"
      },
      "source": [
        "### 一、基本概念\n",
        "\n",
        "1. 朴素贝叶斯（Naive Bayes）\n",
        "   - 基础：贝叶斯定理，特征条件独立假设（假设性较强，也是其被称为Naive的原因）\n",
        "   - 分类：生成模型。先从数据中学习到联合概率分布$P(X,Y)$，然后求得后验概率$P(Y|X)$\n",
        "\n",
        "2. 朴素贝叶斯的算法流程\n",
        "\n",
        "   1. 学习联合概率分布$P(X,Y)$：\n",
        "\n",
        "      因为$P(X,Y) = P(X|Y)·P(Y)$，所以我们期望先分别学习到右侧的两个概率\n",
        "\n",
        "      - $P(Y)$ - class label的先验概率分布\n",
        "      - $P(X|Y)$ - 条件概率分布。其本身拥有**指数级别**的参数。为了简化这一部分，**「朴素贝叶斯对条件概率做出了条件独立性的假设，且因为此假设性较强，故因此牺牲了一定的分类准确率」**\n",
        "\n",
        "      $$\n",
        "      P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},···,X^{(n)}=x^{(n)}|Y=c_k) = \\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)\n",
        "      $$\n",
        "\n",
        "      - 上述公式中，$c_k$代表分类的类别，$x^{(j)}$代表输入$x$的第$j$维度的值。以一个例子来讲：如果我们假设「感冒」和「下雪」是独立的两个特征，则$P(感冒且下雪|天气冷) = P(感冒|天气冷)·P(下雪|天气冷)$，这里的$(感冒且下雪)$其实就可以表示成向量的两个维度，即$(x^{(1)}=1,x^{(2)}=1)$，其中$x^{(1)}$代表是否感冒，取$1$代表感冒，$x^{(2)}$代表是否下雪，取$1$代表下雪。\n",
        "      - 如果我们不进行上述假设，则$P(感冒且下雪|天气冷) \\neq P(感冒|天气冷)·P(下雪|天气冷)$，且我们很难去建模$P(感冒且下雪|天气冷)$，此概率其会有**指数级别的参数量**。\n",
        "      - 最后，我们学习到的联合概率分布为$P(感冒且下雪,天气冷) = P(天气冷)P(感冒且下雪|天气冷) = P(感冒|天气冷)·P(下雪|天气冷)·P(天气冷)$\n",
        "\n",
        "   2. 根据输入$x$，将后验概率最大的类$P(Y=c_k|X=x)$输出，其分类结果为$c_k$\n",
        "      $$\n",
        "      P(Y=c_k|X=x) = \\frac{P(X,Y)}{P(X)} = \\frac{P(Y=c_k)\\prod_{j}P{(X^{(j)}=x^{(j)}|Y=c_k)}}{\\sum_kP(Y=c_k)\\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)}\n",
        "      $$\n",
        "      我们期望得到后验概率最大的类，则上述公式变为\n",
        "      $$\n",
        "      y = f(x) = \\arg\\max_{c_k}P(Y=c_k|X=x) = \\arg\\max_{c_k}\\frac{P(X,Y)}{P(X)} = \\arg\\max_{c_k}\\frac{P(Y=c_k)\\prod_{j}P{(X^{(j)}=x^{(j)}|Y=c_k)}}{\\sum_kP(Y=c_k)\\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)}\n",
        "      $$\n",
        "      且因为上述公式的分母，对不同的$c_k$都是相同的，所以在$\\arg\\max$过程中可以省区，则最后的公式为\n",
        "      $$\n",
        "      y = \\arg\\max_{c_k}P(Y=c_k)\\prod_{j}P{(X^{(j)}=x^{(j)}|Y=c_k)}\n",
        "      $$\n",
        "      **后验概率最大等价于0-1损失函数时的期望风险最小化**\n",
        "\n",
        "### 二、疑难解释\n",
        "\n",
        "1. 监督学习中，生成方法和判别方法的区别\n",
        "   $$\n",
        "   P(Y|X) = \\frac{P(X,Y)}{P(X)}\n",
        "   $$\n",
        "\n",
        "   - 生成方法的特点：根据数据学习**联合概率分布$P(X,Y)$**，然后求出$P(Y|X)$作为预测的模型，及生成模型。\n",
        "     - 为什么叫生成模型：是因为Model学习到了$P(X,Y)$联合概率分布，我们有了联合概率分布其实我们就可以根据对应的输入$X$**生成**对应的$Y$\n",
        "   - 判别方法的特点：根据数据学习**条件概率**$P(Y|X)$或决策函数$f(X)$，直接面对预测/\n",
        "     - 为什么叫判别模型：对给定的输入$X$，应该预测什么样的输出$Y$\n",
        "\n",
        "   > 举例\n",
        "   >\n",
        "   > - 生成式模型：输入历史数据，利用生成模型根据山羊的特征首先学习出一个山羊的模型$P_1(X,Y)$，根据绵羊的特征学习出一个绵羊的模型$P_2(X,Y)$。然后输入一只羊$X$，放入不同的模型，得到两个联合概率$P_1(X,Y),P_2(X,Y)$，哪个联合概率大我们就认为这只输入的羊是什么品种的羊。\n",
        "   > - 判别式模型：输入历史数据，然后学习一个判别模型$P(Y|X)$，根据输入一只羊的特征$X$来判断这只羊是山羊的概率$P(山羊|X)$和绵羊的概率$P(绵羊|X)$\n",
        "\n",
        "   - 总结：判别模型可以根据一只羊的特征直接给出这只羊属于对应种类的概率。生成模型则是根据一只羊的特征，将其和不同生成模型进行比对，最大的概率就是最后的结果。\n",
        "\n",
        "2. 朴素贝叶斯的参数估计\n",
        "\n",
        "   - 估计对象：$P(Y=c_k)$和$P(X^{(j)}=x^{(j)}|Y=c_k)$\n",
        "   - 估计方法：「极大似然估计」和「贝叶斯估计」\n",
        "\n",
        "> 例：根据下表的训练数据学习一个朴素贝叶斯分类器并确定$x = (2,S)^T$的class label $y$。表中$X^{(1)},X^{(2)}$为两个特征。取值的集合分别为$X^{(1)} = \\{1,2,3\\},X^{(2)} = \\{S,M,L\\}$，class label $y = \\{1,-1\\}$\n",
        ">\n",
        "> |           | 1    | 2    | 3    | 4    | 5    | 6    | 7    | 8    | 9    | 10   | 11   | 12   | 13   | 14   | 15   |\n",
        "> | --------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n",
        "> | $X^{(1)}$ | 1    | 1    | 1    | 1    | 1    | 2    | 2    | 2    | 2    | 2    | 3    | 3    | 3    | 3    | 3    |\n",
        "> | $X^{(2)}$ | S    | M    | M    | S    | S    | S    | M    | M    | L    | L    | L    | M    | M    | L    | L    |\n",
        "> | Y         | -1   | -1   | 1    | 1    | -1   | -1   | -1   | 1    | 1    | 1    | 1    | 1    | 1    | 1    | -1   |\n",
        "\n",
        "3. 参数估计 - 极大似然估计\n",
        "\n",
        "   - 如何估计$P(Y=c_k)$\n",
        "     $$\n",
        "     P(Y=c_k) = \\frac{\\sum_{i=1}^N I(y_i=c_k)}{N}\n",
        "     $$\n",
        "     其中$I()$为指示函数，通俗解释这个公式就是直接去看$Y$这一行，根据训练数据的$Y$这一行的数量来定义$P(Y=c_k)$\n",
        "\n",
        "     $P(Y=1) = \\frac{9}{15}, P(Y=-1) = \\frac{6}{15}$\n",
        "\n",
        "   - 如何估计$P(X^{(j)}=x^{(j)}|Y=c_k)$\n",
        "     $$\n",
        "     P(X^{(j)} = a_{jl}| Y=c_k)=\\frac{\\sum_{i=1}^NI(x_{i}^{(j)}=a_{jl},y_i=c_k)}{\\sum_{i=1}^NI(y_i=c_k)}\n",
        "     $$\n",
        "     其中$x_i^{(j)}$是第$i$个样本的第$j$个特征，$a_{jl}$是第$j$个特征可能取的第$l$个值\n",
        "\n",
        "     **通俗解释上述公式**\n",
        "\n",
        "     我们这时候期望估计的是：$x=(2,S)^T$，我们的class label有两种可能，$y = \\{1,-1\\}$。\n",
        "\n",
        "     在$y=1$时候，我们希望估计以下两项$P(X^{(1)} = 2|Y=1),P(X^{(2)}= S|Y=1)$\n",
        "\n",
        "     - 其中$P(X^{(1)} = 2|Y=1)$可以这样估计：在$Y=1$的前提下，我们发现有9列满足要求。在这9列中，$X^{(1)} = 2$的有3列，则$P(X^{(1)} = 2|Y=1) = \\frac{3}{9}$\n",
        "     - 其中$P(X^{(2)} = S|Y=1)$可以这样估计：在$Y=1$的前提下，我们发现有9列满足要求。在这9列中，$X^{(2)} = S$的有1列，则$P(X^{(2)} = S|Y=1) = \\frac{1}{9}$\n",
        "\n",
        "     在$y=-1$时候，我们希望估计以下两项$P(X^{(1)} = 2|Y=-1),P(X^{(2)}= S|Y=-1)$\n",
        "\n",
        "     - 其中$P(X^{(1)} = 2|Y=-1)$可以这样估计：在$Y=-1$的前提下，我们发现有6列满足要求。在这6列中，$X^{(1)} = 2$的有2列，则$P(X^{(1)} = 2|Y=-1) = \\frac{2}{6}$\n",
        "     - 其中$P(X^{(2)} = S|Y=-1)$可以这样估计：在$Y=-1$的前提下，我们发现有6列满足要求。在这6列中，$X^{(2)} = S$的有3列，则$P(X^{(2)} = S|Y=1) = \\frac{3}{6}$\n",
        "\n",
        "   - 我们的目标函数为（见上文）：$y = \\arg\\max_{c_k}P(Y=c_k)\\prod_{j}P{(X^{(j)}=x^{(j)}|Y=c_k)}$\n",
        "\n",
        "     在$Y=1$的情况下：\n",
        "     $$\n",
        "     P(Y=1)P(X^{(1)}=2|Y=1)P(X^{(2)}=S|Y=1) = \\frac{9}{15}·\\frac{3}{9}·\\frac{1}{9} = \\frac{1}{45}\n",
        "     $$\n",
        "     在$Y=-1$的情况下：\n",
        "     $$\n",
        "     P(Y=-1)P(X^{(1)}=2|Y=-1)P(X^{(2)}=S|Y=-1) = \\frac{6}{15}·\\frac{2}{6}·\\frac{3}{6} = \\frac{1}{15}\n",
        "     $$\n",
        "     显然$Y = c_k = -1$时，对应的后验概率较大，则$y = -1$，也就是我们预测得到的class label\n",
        "\n",
        "4. 参数估计 - 贝叶斯估计（拉普拉斯平滑解决极大似然估计概率的问题）\n",
        "\n",
        "   - **极大似然估计的问题**：可能出现所需要估计的概率值为0的情况 => 使用拉普拉斯平滑（Laplacian smoothing）估计概率\n",
        "\n",
        "     拉普拉斯平滑估计就是在每一项概率计算的分子分母上都同时加上$\\lambda,\\text{where }\\lambda > 0$，$\\lambda = 0$时为极大似然估计，常取$\\lambda = 1$为拉普拉斯平滑估计，**「这里的$\\lambda$实际上就是一种先验假设」**\n",
        "\n",
        "   - 如何估计$P(Y=c_k)$\n",
        "     $$\n",
        "     P(Y=c_k) = \\frac{\\sum_{i=1}^N I(y_i=c_k)+\\lambda}{N+K\\lambda}\n",
        "     $$\n",
        "     其中$K$代表class label的总数，这里为$Y= \\{1,-1\\}$，所以$K = 2$\n",
        "\n",
        "     $P(Y=1)=\\frac{10}{17},P(Y=-1) = \\frac{7}{17}$\n",
        "\n",
        "   - 如何估计$P(X^{(j)}=x^{(j)}|Y=c_k)$\n",
        "     $$\n",
        "     P(X^{(j)} = a_{jl}| Y=c_k)=\\frac{\\sum_{i=1}^NI(x_{i}^{(j)}=a_{jl},y_i=c_k)+\\lambda}{\\sum_{i=1}^NI(y_i=c_k)+S_j\\lambda}\n",
        "     $$\n",
        "     其中$S_j$代表第$j$个特征总共有几种可能的取值，这里$X^{(1)} = \\{1,2,3\\}$，所以$S_1 = 3$，$X^{(2)} = \\{S,M,L\\}$，所以$S_2 = 3$\n",
        "\n",
        "   - 我们的目标函数为（见上文）：$y = \\arg\\max_{c_k}P(Y=c_k)\\prod_{j}P{(X^{(j)}=x^{(j)}|Y=c_k)}$\n",
        "\n",
        "     在$Y=1$的情况下：\n",
        "\n",
        "   $$\n",
        "   P(Y=1)P(X^{(1)}=2|Y=1)P(X^{(2)}=S|Y=1) = \\frac{9+1}{15+2·1}·\\frac{3+1}{9+3·1}·\\frac{1+1}{9+3·1} = \\frac{5}{153}=0.0327\n",
        "   $$\n",
        "\n",
        "   ​\t\t在$Y=-1$的情况下：\n",
        "   $$\n",
        "   P(Y=-1)P(X^{(1)}=2|Y=-1)P(X^{(2)}=S|Y=-1) = \\frac{6+1}{15+2·1}·\\frac{2+1}{6+3·1}·\\frac{3+1}{6+3·1} = \\frac{28}{459}=0.0610\n",
        "   $$\n",
        "   ​\t\t显然$Y = c_k = -1$时，对应的后验概率较大，则$y = -1$，也就是我们预测得到的class label\n",
        "\n",
        "### 三、习题\n",
        "\n",
        "> 用极大似然估计法推出朴素贝叶斯法中的概率估计公式$(4.8)$及公式$(4.9)$\n",
        "\n",
        "1. 证明公式$(4.8)$\n",
        "   $$\n",
        "   P(Y=c_k) = \\frac{\\sum_{i=1}^N I(y_i=c_k)}{N} \\tag{4.8}\n",
        "   $$\n",
        "   假设$P(Y=c_k)$的概率为$p$，且$y_i=c_k$的情况总共有$m$个\n",
        "\n",
        "   似然函数定义如下：\n",
        "   $$\n",
        "   L(p) = C_N^mp^m(1-p)^{(N-m)}\n",
        "   $$\n",
        "   对上述该公式求导\n",
        "   $$\n",
        "   \\frac{dL(p)}{dp} = C_{N}^m[mp^{m-1}(1-p)^{(N-M)}-(N-m)p^m(1-p)^{(N-m-1)}] \\\\ = C_N^m[p^{(m-1)}(1-p)^{(N-m-1)}(m-Np)]\n",
        "   $$\n",
        "   令$\\frac{dL(p)}{dp}=0$，得到$p = 0, p = 1, p=\\frac{m}{N}$\n",
        "\n",
        "   显然最后一个符合条件，此时\n",
        "   $$\n",
        "   P(Y=c_k)=p=\\frac{m}{N}=\\frac{\\sum_{i=1}^N I(y_i=c_k)}{N} \n",
        "   $$\n",
        "\n",
        "2. 证明公式$(4.9)$\n",
        "   $$\n",
        "   P(X^{(j)} = a_{jl}| Y=c_k)=\\frac{\\sum_{i=1}^NI(x_{i}^{(j)}=a_{jl},y_i=c_k)}{\\sum_{i=1}^NI(y_i=c_k)} \\tag{4.9}\n",
        "   $$\n",
        "   \n",
        "\n",
        "   令$P(X^{(j)} = a_{jl}| Y=c_k) = p$，令$m = \\sum_{i=1}^NI(y_i=c_k) $, $q = \\sum_{i=1}^NI(x_{i}^{(j)}=a_{jl},y_i=c_k)$\n",
        "\n",
        "   似然函数如下\n",
        "   $$\n",
        "   L(p) = C_m^qp^q(i-p)^{(m-q)}\n",
        "   $$\n",
        "   求微分令导数等0，得\n",
        "   $$\n",
        "   p =0, p=1,p=\\frac{q}{m}\n",
        "   $$\n",
        "   显然$p = q/m$满足题意，则\n",
        "   $$\n",
        "   P(X^{(j)} = a_{jl}| Y=c_k)=p = \\frac{q}{m}=\\frac{\\sum_{i=1}^NI(x_{i}^{(j)}=a_{jl},y_i=c_k)}{\\sum_{i=1}^NI(y_i=c_k)} \\tag{4.9}\n",
        "   $$\n",
        "\n",
        "> 用贝叶斯估计法推出朴素贝叶斯法中的概率估计公式$(4.10)$及公式$(4.11)$\n",
        "\n",
        "1. 证明公式$(4.11)$\n",
        "   $$\n",
        "   P(Y=c_k) = \\frac{\\sum_{i=1}^N I(y_i=c_k)+\\lambda}{N+K\\lambda}\n",
        "   $$\n",
        "   这里的$\\lambda$实际上是一种先验概率，在没有任何信息的情况下，我们可以假设先验概率为均匀概率（即每个事件的概率都是相同的），得\n",
        "   $$\n",
        "   p = \\frac{1}{K} \\rightarrow pK-1=0 \\tag{1}\n",
        "   $$\n",
        "   显然该先验概率的极大似然估计为\n",
        "   $$\n",
        "   p = \\frac{\\sum_{i=1}^nI(y_i=c_k)}{N} \\rightarrow pN-\\sum_{i=1}^nI(y_i=c_k)=0 \\tag{2}\n",
        "   $$\n",
        "   因为$0·\\lambda+0 = 0$\n",
        "\n",
        "   所以存在参数$\\lambda$让$(1)·\\lambda+(2)=0$，代入解得\n",
        "   $$\n",
        "   P(Y=c_k) = \\frac{\\sum_{i=1}^N I(y_i=c_k)+\\lambda}{N+K\\lambda}\n",
        "   $$\n",
        "\n",
        "2. 证明公式$(4.10)$\n",
        "   $$\n",
        "   P(X^{(j)} = a_{jl}| Y=c_k)=\\frac{\\sum_{i=1}^NI(x_{i}^{(j)}=a_{jl},y_i=c_k)+\\lambda}{\\sum_{i=1}^NI(y_i=c_k)+S_j\\lambda}\n",
        "   $$\n",
        "   由上一个证明公式$(4.11)$可以得到\n",
        "   $$\n",
        "   P(Y=c_k,x^{(j)}=a_{jl}) = \\frac{\\sum_{i=1}^N I(y_i=c_k,x_i^{j}=a_{jl})+\\lambda}{N+KS_j\\lambda} \\tag{3}\n",
        "   $$\n",
        "   故\n",
        "   $$\n",
        "   P(X^{(j)} = a_{jl}| Y=c_k)=\\frac{P(Y=c_k,x^{(j)}=a_{jl})}{P(y_j=c_k)} \\ (分子用(3)替换，分母用(1)替换) \\\\ = \\frac{\\frac{\\sum_{i=1}^N I(y_i=c_k,x_i^{j}=a_{jl})+\\lambda}{N+KS_j\\lambda}}{ \\frac{\\sum_{i=1}^N I(y_i=c_k)+\\lambda}{N+K\\lambda}}\\ (\\lambda可以取任意值，故取\\lambda = S_j\\lambda) \\\\ = \\frac{\\sum_{i=1}^NI(x_{i}^{(j)}=a_{jl},y_i=c_k)+\\lambda}{\\sum_{i=1}^NI(y_i=c_k)+S_j\\lambda}\n",
        "   $$\n",
        "   "
      ]
    }
  ]
}