{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Advanced]_Variational_Autoencoder.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNEL4SWiaV0FDXBlQHUgLPZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vvvu/potential-chainsaw/blob/main/pytorch-tutorial/%5BAdvanced%5D_Variational_Autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE1t7843V4QY"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f11srGIxdNFV"
      },
      "source": [
        "### PyTorch中，`nn`与`nn.functional`有什么区别\n",
        "\n",
        "1. 两者的相同之处\n",
        "   - `nn.()`和`nn.functional.()`的实际功能是相同的，即`nn.Conv2d()`与`nn.functional.conv2d()`都是进行卷积操作，`nn.Dropout()`和`nn.functional.dropout()`都是进行**dropout**操作\n",
        "   - 运行效率也**几乎相同**\n",
        "\n",
        "2. 两者的不同之处\n",
        "\n",
        "   - `nn.functional.()`是函数结构，而`nn.()`是`nn.functional.()`的**类封装**，且`nn.()`继承于一个公共祖先`nn.Module()`，这就导致`nn.()`除了有`nn.functional.()`的功能以为，**还有`nn.Module()`相关的属性和方法，例如`train(),eval(),load_state_dict(),state_dict()`**等\n",
        "   - 调用方式不同，`nn.()`需要实例化并传入参数，`nn.functional.()`同时传入输入数据和`weight,bias`等其他参数\n",
        "   - `nn.()`继承于`nn.Module()`，**这让它可以很好地与`nn.Sequential()`连用**，而`nn.functional.()`则不可以\n",
        "   - `nn.()`不需要自己定义和管理`Weight`，而`nn.functional.()`需要自己定义`Weight`，每次调用都需要手动传入`Weight`，**不利于代码复用**\n",
        "\n",
        "3. 总结：两者的应用场景\n",
        "\n",
        "   > 这个问题依赖于你要解决你问题的复杂度和个人风格喜好。在`nn.()`不能满足你的功能需求时，`nn.functional.()`是更佳的选择，因为`nn.functional.()`更加的灵活(更加接近底层），你可以在其基础上定义出自己想要的功能。\n",
        "\n",
        "   总而言之，可以使用`nn.()`尽量使用，不可以的情况再换`nn.functional.()`。这样可以更好表达网络的层次关系，且所有的组件都继承于`nn.Module()`，更为和谐统一。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBEF3cWOWbVa"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Create a directory if not exists\n",
        "sample_dir = 'samples'\n",
        "if not os.path.exists(sample_dir):\n",
        "  os.makedirs(sample_dir)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uvSCoE6Wp_u"
      },
      "source": [
        "# Hyper Parameters\n",
        "image_size = 28 * 28\n",
        "h_dim = 400\n",
        "z_dim = 20 # z-space/latent space\n",
        "num_epochs = 15\n",
        "batch_size = 128\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# MNIST dataset\n",
        "dataset = torchvision.datasets.MNIST(root = './data',\n",
        "                                     train = True,\n",
        "                                     transform = transforms.ToTensor(),\n",
        "                                     download = True)\n",
        "\n",
        "# Data Loader\n",
        "data_loader = torch.utils.data.DataLoader(dataset = dataset,\n",
        "                                          batch_size = batch_size,\n",
        "                                          shuffle = True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0d5ZpL0XFqZ"
      },
      "source": [
        "# VAE model\n",
        "class VAE(nn.Module):\n",
        "  def __init__(self, image_size = 784, h_dim = 400, z_dim = 20):\n",
        "    super(VAE, self).__init__()\n",
        "    self.fc1 = nn.Linear(image_size, h_dim)\n",
        "    self.fc2 = nn.Linear(h_dim, z_dim)\n",
        "    self.fc3 = nn.Linear(h_dim, z_dim)\n",
        "    self.fc4 = nn.Linear(z_dim, h_dim)\n",
        "    self.fc5 = nn.Linear(h_dim, image_size)\n",
        "\n",
        "  def encode(self, x):\n",
        "    h = F.relu(self.fc1(x))\n",
        "    return self.fc2(h), self.fc3(h)\n",
        "\n",
        "  def reparameterize(self, mu, log_var):\n",
        "    std = torch.exp(log_var/2)\n",
        "    eps = torch.randn_like(std)\n",
        "    '''\n",
        "    - torch.randn_like(): Returns a tensor with the same size as `input` that\n",
        "    is filled with random numbers from a normal distribution with mean 0 and\n",
        "    variance 1. Actually, `torch.randn_like(input)` is equivalent to `torch.randn(input.size())`\n",
        "    '''\n",
        "    return mu + eps * std\n",
        "\n",
        "  def decode(self, z):\n",
        "    h = F.relu(self.fc4(z))\n",
        "    return F.sigmoid(self.fc5(h))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mu, log_var = self.encode(x)\n",
        "    z = self.reparameterize(mu, log_var)\n",
        "    x_reconst = self.decode(z)\n",
        "    return x_reconst, mu, log_var"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAb-EayPYJ9_"
      },
      "source": [
        "model = VAE().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnpunZsHYPat",
        "outputId": "44a9c068-18b6-4d59-d5d0-973a4c1f79d3"
      },
      "source": [
        "# Start training\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (x, _) in enumerate(data_loader):\n",
        "    # Forward pass\n",
        "    x = x.to(device).view(-1, image_size)\n",
        "    x_reconst, mu, log_var = model(x)\n",
        "\n",
        "    # Compute reconstruction loss and KL divergence\n",
        "    reconst_loss = F.binary_cross_entropy(x_reconst, x, size_average=False)\n",
        "    kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "\n",
        "    # Backprop and optimize\n",
        "    loss = reconst_loss + kl_div\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i + 1) % 10 == 0:\n",
        "      print(\"Epoch[{}/{}], Step [{}/{}], Reconst Loss: {:.4f}, KL Div: {:.4f}\"\n",
        "            .format(epoch + 1, num_epochs, i + 1, len(data_loader), \n",
        "                    reconst_loss.item(), kl_div.item()))\n",
        "      \n",
        "  with torch.no_grad():\n",
        "    # Save the sampled images\n",
        "    z = torch.randn(batch_size, z_dim).to(device)\n",
        "    out = model.decode(z).view(-1, 1, 28, 28)\n",
        "    save_image(out, os.path.join(sample_dir, 'sampled-{}.png'.format(epoch + 1)))\n",
        "\n",
        "    # Save the reconstructed images\n",
        "    out, _, _ = model(x)\n",
        "    x_concat = torch.cat([x.view(-1, 1, 28, 28), out.view(-1, 1, 28, 28)], dim = 3)\n",
        "    save_image(x_concat, os.path.join(sample_dir, 'reconst-{}.png'.format(epoch + 1)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch[1/15], Step [10/469], Reconst Loss: 37055.2930, KL Div: 3659.2480\n",
            "Epoch[1/15], Step [20/469], Reconst Loss: 30148.3438, KL Div: 1020.1989\n",
            "Epoch[1/15], Step [30/469], Reconst Loss: 27276.3477, KL Div: 1281.7863\n",
            "Epoch[1/15], Step [40/469], Reconst Loss: 26898.0918, KL Div: 715.1939\n",
            "Epoch[1/15], Step [50/469], Reconst Loss: 27114.1270, KL Div: 691.8860\n",
            "Epoch[1/15], Step [60/469], Reconst Loss: 26966.9863, KL Div: 771.9081\n",
            "Epoch[1/15], Step [70/469], Reconst Loss: 24098.0703, KL Div: 1114.8950\n",
            "Epoch[1/15], Step [80/469], Reconst Loss: 23949.5898, KL Div: 1023.3667\n",
            "Epoch[1/15], Step [90/469], Reconst Loss: 23285.9434, KL Div: 1291.3933\n",
            "Epoch[1/15], Step [100/469], Reconst Loss: 21779.4297, KL Div: 1349.2325\n",
            "Epoch[1/15], Step [110/469], Reconst Loss: 20397.9668, KL Div: 1556.3445\n",
            "Epoch[1/15], Step [120/469], Reconst Loss: 20621.3711, KL Div: 1605.1548\n",
            "Epoch[1/15], Step [130/469], Reconst Loss: 19545.6387, KL Div: 1715.9426\n",
            "Epoch[1/15], Step [140/469], Reconst Loss: 19769.1680, KL Div: 1842.9268\n",
            "Epoch[1/15], Step [150/469], Reconst Loss: 18679.0469, KL Div: 1878.1865\n",
            "Epoch[1/15], Step [160/469], Reconst Loss: 18269.7344, KL Div: 1757.1970\n",
            "Epoch[1/15], Step [170/469], Reconst Loss: 19055.3398, KL Div: 1999.2783\n",
            "Epoch[1/15], Step [180/469], Reconst Loss: 18274.8945, KL Div: 1816.5471\n",
            "Epoch[1/15], Step [190/469], Reconst Loss: 18272.5977, KL Div: 1948.2714\n",
            "Epoch[1/15], Step [200/469], Reconst Loss: 17775.1797, KL Div: 1867.4255\n",
            "Epoch[1/15], Step [210/469], Reconst Loss: 17070.1465, KL Div: 1963.2202\n",
            "Epoch[1/15], Step [220/469], Reconst Loss: 17646.8867, KL Div: 1961.7277\n",
            "Epoch[1/15], Step [230/469], Reconst Loss: 17093.0352, KL Div: 1900.3414\n",
            "Epoch[1/15], Step [240/469], Reconst Loss: 16865.6289, KL Div: 1954.7638\n",
            "Epoch[1/15], Step [250/469], Reconst Loss: 16690.4883, KL Div: 2165.2017\n",
            "Epoch[1/15], Step [260/469], Reconst Loss: 16547.9473, KL Div: 2102.7217\n",
            "Epoch[1/15], Step [270/469], Reconst Loss: 16015.8691, KL Div: 2215.3325\n",
            "Epoch[1/15], Step [280/469], Reconst Loss: 16914.7812, KL Div: 2170.8752\n",
            "Epoch[1/15], Step [290/469], Reconst Loss: 16463.4883, KL Div: 2252.3914\n",
            "Epoch[1/15], Step [300/469], Reconst Loss: 15439.5000, KL Div: 2208.9453\n",
            "Epoch[1/15], Step [310/469], Reconst Loss: 15323.2646, KL Div: 2234.6387\n",
            "Epoch[1/15], Step [320/469], Reconst Loss: 15063.4141, KL Div: 2420.8403\n",
            "Epoch[1/15], Step [330/469], Reconst Loss: 15221.5547, KL Div: 2202.0784\n",
            "Epoch[1/15], Step [340/469], Reconst Loss: 15096.8125, KL Div: 2448.4736\n",
            "Epoch[1/15], Step [350/469], Reconst Loss: 15122.1445, KL Div: 2379.2893\n",
            "Epoch[1/15], Step [360/469], Reconst Loss: 14921.9961, KL Div: 2603.3259\n",
            "Epoch[1/15], Step [370/469], Reconst Loss: 14301.3818, KL Div: 2421.7109\n",
            "Epoch[1/15], Step [380/469], Reconst Loss: 14416.7041, KL Div: 2503.3435\n",
            "Epoch[1/15], Step [390/469], Reconst Loss: 14426.4238, KL Div: 2600.1348\n",
            "Epoch[1/15], Step [400/469], Reconst Loss: 14561.9482, KL Div: 2518.3350\n",
            "Epoch[1/15], Step [410/469], Reconst Loss: 14379.0332, KL Div: 2572.9692\n",
            "Epoch[1/15], Step [420/469], Reconst Loss: 14095.5566, KL Div: 2583.6748\n",
            "Epoch[1/15], Step [430/469], Reconst Loss: 14027.4434, KL Div: 2593.9819\n",
            "Epoch[1/15], Step [440/469], Reconst Loss: 14049.5205, KL Div: 2547.8494\n",
            "Epoch[1/15], Step [450/469], Reconst Loss: 13976.3359, KL Div: 2626.5542\n",
            "Epoch[1/15], Step [460/469], Reconst Loss: 13701.4844, KL Div: 2734.4714\n",
            "Epoch[2/15], Step [10/469], Reconst Loss: 14162.8926, KL Div: 2618.6885\n",
            "Epoch[2/15], Step [20/469], Reconst Loss: 12892.5664, KL Div: 2626.3926\n",
            "Epoch[2/15], Step [30/469], Reconst Loss: 13566.1992, KL Div: 2650.1748\n",
            "Epoch[2/15], Step [40/469], Reconst Loss: 13434.8789, KL Div: 2730.8677\n",
            "Epoch[2/15], Step [50/469], Reconst Loss: 13554.0547, KL Div: 2723.8066\n",
            "Epoch[2/15], Step [60/469], Reconst Loss: 14055.8164, KL Div: 2582.5312\n",
            "Epoch[2/15], Step [70/469], Reconst Loss: 13089.9678, KL Div: 2650.6658\n",
            "Epoch[2/15], Step [80/469], Reconst Loss: 13042.7715, KL Div: 2630.0596\n",
            "Epoch[2/15], Step [90/469], Reconst Loss: 13369.8252, KL Div: 2846.8989\n",
            "Epoch[2/15], Step [100/469], Reconst Loss: 12910.4746, KL Div: 2777.3611\n",
            "Epoch[2/15], Step [110/469], Reconst Loss: 13202.6523, KL Div: 2719.2896\n",
            "Epoch[2/15], Step [120/469], Reconst Loss: 13299.6641, KL Div: 2740.7795\n",
            "Epoch[2/15], Step [130/469], Reconst Loss: 13426.5947, KL Div: 2702.6929\n",
            "Epoch[2/15], Step [140/469], Reconst Loss: 13489.5068, KL Div: 2730.0825\n",
            "Epoch[2/15], Step [150/469], Reconst Loss: 12996.8418, KL Div: 2748.1167\n",
            "Epoch[2/15], Step [160/469], Reconst Loss: 12667.9590, KL Div: 2975.0444\n",
            "Epoch[2/15], Step [170/469], Reconst Loss: 12647.9844, KL Div: 2783.2751\n",
            "Epoch[2/15], Step [180/469], Reconst Loss: 13147.7041, KL Div: 2815.7222\n",
            "Epoch[2/15], Step [190/469], Reconst Loss: 12984.4189, KL Div: 2807.7004\n",
            "Epoch[2/15], Step [200/469], Reconst Loss: 12893.8936, KL Div: 2970.8667\n",
            "Epoch[2/15], Step [210/469], Reconst Loss: 12697.6367, KL Div: 2848.0698\n",
            "Epoch[2/15], Step [220/469], Reconst Loss: 12677.4082, KL Div: 2938.1819\n",
            "Epoch[2/15], Step [230/469], Reconst Loss: 12563.4033, KL Div: 2869.1550\n",
            "Epoch[2/15], Step [240/469], Reconst Loss: 13080.5879, KL Div: 2974.1372\n",
            "Epoch[2/15], Step [250/469], Reconst Loss: 12335.5957, KL Div: 2832.3582\n",
            "Epoch[2/15], Step [260/469], Reconst Loss: 12488.0723, KL Div: 2974.1953\n",
            "Epoch[2/15], Step [270/469], Reconst Loss: 11965.2676, KL Div: 2848.9304\n",
            "Epoch[2/15], Step [280/469], Reconst Loss: 11885.6016, KL Div: 3023.6726\n",
            "Epoch[2/15], Step [290/469], Reconst Loss: 11914.3555, KL Div: 2888.5356\n",
            "Epoch[2/15], Step [300/469], Reconst Loss: 12630.7578, KL Div: 2969.1162\n",
            "Epoch[2/15], Step [310/469], Reconst Loss: 11536.5176, KL Div: 2931.9666\n",
            "Epoch[2/15], Step [320/469], Reconst Loss: 12120.1758, KL Div: 2782.7891\n",
            "Epoch[2/15], Step [330/469], Reconst Loss: 11869.2793, KL Div: 2828.8052\n",
            "Epoch[2/15], Step [340/469], Reconst Loss: 12396.2793, KL Div: 2925.8301\n",
            "Epoch[2/15], Step [350/469], Reconst Loss: 12700.2539, KL Div: 3031.9932\n",
            "Epoch[2/15], Step [360/469], Reconst Loss: 12210.6865, KL Div: 2876.0771\n",
            "Epoch[2/15], Step [370/469], Reconst Loss: 11925.5430, KL Div: 3056.2700\n",
            "Epoch[2/15], Step [380/469], Reconst Loss: 12365.1758, KL Div: 3014.4578\n",
            "Epoch[2/15], Step [390/469], Reconst Loss: 12511.2578, KL Div: 2947.1025\n",
            "Epoch[2/15], Step [400/469], Reconst Loss: 11743.8262, KL Div: 2892.2427\n",
            "Epoch[2/15], Step [410/469], Reconst Loss: 12452.0215, KL Div: 2916.6165\n",
            "Epoch[2/15], Step [420/469], Reconst Loss: 12268.8594, KL Div: 2998.9961\n",
            "Epoch[2/15], Step [430/469], Reconst Loss: 12541.0234, KL Div: 2994.1521\n",
            "Epoch[2/15], Step [440/469], Reconst Loss: 12106.5557, KL Div: 2888.3206\n",
            "Epoch[2/15], Step [450/469], Reconst Loss: 12227.0195, KL Div: 2881.8901\n",
            "Epoch[2/15], Step [460/469], Reconst Loss: 11357.3311, KL Div: 2874.5620\n",
            "Epoch[3/15], Step [10/469], Reconst Loss: 11774.4492, KL Div: 2957.0369\n",
            "Epoch[3/15], Step [20/469], Reconst Loss: 11858.9639, KL Div: 2976.4736\n",
            "Epoch[3/15], Step [30/469], Reconst Loss: 11917.2734, KL Div: 3039.6218\n",
            "Epoch[3/15], Step [40/469], Reconst Loss: 12143.9375, KL Div: 3106.5825\n",
            "Epoch[3/15], Step [50/469], Reconst Loss: 11775.0449, KL Div: 3057.1182\n",
            "Epoch[3/15], Step [60/469], Reconst Loss: 12024.4902, KL Div: 2900.8447\n",
            "Epoch[3/15], Step [70/469], Reconst Loss: 11702.5137, KL Div: 3012.9658\n",
            "Epoch[3/15], Step [80/469], Reconst Loss: 12010.2031, KL Div: 3080.8315\n",
            "Epoch[3/15], Step [90/469], Reconst Loss: 11596.8809, KL Div: 2950.5063\n",
            "Epoch[3/15], Step [100/469], Reconst Loss: 11826.8184, KL Div: 2855.1499\n",
            "Epoch[3/15], Step [110/469], Reconst Loss: 11716.5635, KL Div: 3071.1267\n",
            "Epoch[3/15], Step [120/469], Reconst Loss: 11748.6113, KL Div: 3008.6877\n",
            "Epoch[3/15], Step [130/469], Reconst Loss: 11332.5449, KL Div: 3027.1577\n",
            "Epoch[3/15], Step [140/469], Reconst Loss: 11798.2734, KL Div: 2988.8445\n",
            "Epoch[3/15], Step [150/469], Reconst Loss: 12184.2295, KL Div: 3047.4036\n",
            "Epoch[3/15], Step [160/469], Reconst Loss: 11494.6895, KL Div: 2920.9517\n",
            "Epoch[3/15], Step [170/469], Reconst Loss: 11719.4844, KL Div: 3094.8315\n",
            "Epoch[3/15], Step [180/469], Reconst Loss: 11523.4297, KL Div: 3080.4292\n",
            "Epoch[3/15], Step [190/469], Reconst Loss: 11058.0332, KL Div: 2844.3247\n",
            "Epoch[3/15], Step [200/469], Reconst Loss: 11791.9814, KL Div: 3103.9209\n",
            "Epoch[3/15], Step [210/469], Reconst Loss: 12102.3809, KL Div: 3103.5005\n",
            "Epoch[3/15], Step [220/469], Reconst Loss: 11823.5049, KL Div: 3037.0178\n",
            "Epoch[3/15], Step [230/469], Reconst Loss: 11205.8965, KL Div: 3147.0054\n",
            "Epoch[3/15], Step [240/469], Reconst Loss: 11569.4141, KL Div: 3078.5254\n",
            "Epoch[3/15], Step [250/469], Reconst Loss: 11975.6387, KL Div: 3114.0439\n",
            "Epoch[3/15], Step [260/469], Reconst Loss: 12019.0498, KL Div: 3019.5779\n",
            "Epoch[3/15], Step [270/469], Reconst Loss: 11340.1055, KL Div: 3085.7852\n",
            "Epoch[3/15], Step [280/469], Reconst Loss: 11644.4346, KL Div: 2996.2246\n",
            "Epoch[3/15], Step [290/469], Reconst Loss: 11722.8691, KL Div: 3079.9946\n",
            "Epoch[3/15], Step [300/469], Reconst Loss: 11513.0957, KL Div: 3257.0356\n",
            "Epoch[3/15], Step [310/469], Reconst Loss: 11687.9922, KL Div: 3043.6655\n",
            "Epoch[3/15], Step [320/469], Reconst Loss: 11810.0059, KL Div: 3198.0654\n",
            "Epoch[3/15], Step [330/469], Reconst Loss: 11270.9023, KL Div: 3092.6885\n",
            "Epoch[3/15], Step [340/469], Reconst Loss: 12019.7998, KL Div: 3108.6230\n",
            "Epoch[3/15], Step [350/469], Reconst Loss: 10767.2480, KL Div: 3041.8938\n",
            "Epoch[3/15], Step [360/469], Reconst Loss: 11199.8398, KL Div: 3048.8418\n",
            "Epoch[3/15], Step [370/469], Reconst Loss: 11711.2471, KL Div: 3076.6609\n",
            "Epoch[3/15], Step [380/469], Reconst Loss: 11470.0957, KL Div: 3193.4260\n",
            "Epoch[3/15], Step [390/469], Reconst Loss: 11759.5059, KL Div: 3019.7205\n",
            "Epoch[3/15], Step [400/469], Reconst Loss: 11320.7500, KL Div: 3001.2144\n",
            "Epoch[3/15], Step [410/469], Reconst Loss: 11410.7305, KL Div: 3085.9175\n",
            "Epoch[3/15], Step [420/469], Reconst Loss: 11451.4424, KL Div: 3035.9321\n",
            "Epoch[3/15], Step [430/469], Reconst Loss: 11335.7852, KL Div: 3151.7910\n",
            "Epoch[3/15], Step [440/469], Reconst Loss: 11140.9619, KL Div: 3165.8022\n",
            "Epoch[3/15], Step [450/469], Reconst Loss: 11586.9092, KL Div: 3108.5117\n",
            "Epoch[3/15], Step [460/469], Reconst Loss: 11278.9941, KL Div: 3146.1797\n",
            "Epoch[4/15], Step [10/469], Reconst Loss: 11697.9199, KL Div: 3220.3386\n",
            "Epoch[4/15], Step [20/469], Reconst Loss: 11879.6836, KL Div: 3060.9546\n",
            "Epoch[4/15], Step [30/469], Reconst Loss: 11395.4072, KL Div: 3129.8877\n",
            "Epoch[4/15], Step [40/469], Reconst Loss: 11472.3184, KL Div: 3167.9080\n",
            "Epoch[4/15], Step [50/469], Reconst Loss: 11136.0977, KL Div: 3158.4258\n",
            "Epoch[4/15], Step [60/469], Reconst Loss: 11221.5781, KL Div: 3068.0220\n",
            "Epoch[4/15], Step [70/469], Reconst Loss: 11295.8633, KL Div: 3026.9360\n",
            "Epoch[4/15], Step [80/469], Reconst Loss: 11197.4434, KL Div: 3137.6030\n",
            "Epoch[4/15], Step [90/469], Reconst Loss: 10870.1240, KL Div: 3127.2676\n",
            "Epoch[4/15], Step [100/469], Reconst Loss: 10937.4219, KL Div: 3171.3877\n",
            "Epoch[4/15], Step [110/469], Reconst Loss: 11247.5068, KL Div: 3130.3965\n",
            "Epoch[4/15], Step [120/469], Reconst Loss: 10995.2256, KL Div: 3066.8496\n",
            "Epoch[4/15], Step [130/469], Reconst Loss: 11071.0439, KL Div: 3230.8848\n",
            "Epoch[4/15], Step [140/469], Reconst Loss: 11189.4453, KL Div: 3083.3616\n",
            "Epoch[4/15], Step [150/469], Reconst Loss: 11247.9316, KL Div: 3002.1726\n",
            "Epoch[4/15], Step [160/469], Reconst Loss: 10978.0488, KL Div: 3227.6035\n",
            "Epoch[4/15], Step [170/469], Reconst Loss: 11110.5264, KL Div: 3118.3491\n",
            "Epoch[4/15], Step [180/469], Reconst Loss: 11243.6113, KL Div: 3093.3489\n",
            "Epoch[4/15], Step [190/469], Reconst Loss: 11314.2656, KL Div: 3157.8425\n",
            "Epoch[4/15], Step [200/469], Reconst Loss: 10907.0830, KL Div: 3097.3516\n",
            "Epoch[4/15], Step [210/469], Reconst Loss: 10776.5322, KL Div: 3144.5205\n",
            "Epoch[4/15], Step [220/469], Reconst Loss: 10497.0273, KL Div: 3111.9517\n",
            "Epoch[4/15], Step [230/469], Reconst Loss: 11078.0801, KL Div: 3112.9546\n",
            "Epoch[4/15], Step [240/469], Reconst Loss: 11069.8330, KL Div: 3202.7751\n",
            "Epoch[4/15], Step [250/469], Reconst Loss: 11223.5215, KL Div: 3087.0933\n",
            "Epoch[4/15], Step [260/469], Reconst Loss: 11062.2988, KL Div: 3125.0654\n",
            "Epoch[4/15], Step [270/469], Reconst Loss: 10836.7578, KL Div: 3017.9961\n",
            "Epoch[4/15], Step [280/469], Reconst Loss: 10845.3008, KL Div: 3069.3706\n",
            "Epoch[4/15], Step [290/469], Reconst Loss: 10284.9062, KL Div: 3056.6201\n",
            "Epoch[4/15], Step [300/469], Reconst Loss: 11016.0977, KL Div: 3088.1553\n",
            "Epoch[4/15], Step [310/469], Reconst Loss: 10922.7939, KL Div: 3066.9204\n",
            "Epoch[4/15], Step [320/469], Reconst Loss: 11239.7676, KL Div: 2995.7061\n",
            "Epoch[4/15], Step [330/469], Reconst Loss: 10911.0391, KL Div: 3179.8989\n",
            "Epoch[4/15], Step [340/469], Reconst Loss: 12059.0605, KL Div: 3280.3491\n",
            "Epoch[4/15], Step [350/469], Reconst Loss: 10751.1348, KL Div: 3119.9785\n",
            "Epoch[4/15], Step [360/469], Reconst Loss: 10719.0000, KL Div: 3224.5557\n",
            "Epoch[4/15], Step [370/469], Reconst Loss: 11117.2305, KL Div: 3062.5852\n",
            "Epoch[4/15], Step [380/469], Reconst Loss: 11372.1699, KL Div: 3159.8110\n",
            "Epoch[4/15], Step [390/469], Reconst Loss: 10119.1797, KL Div: 3077.7439\n",
            "Epoch[4/15], Step [400/469], Reconst Loss: 10889.4746, KL Div: 3230.9878\n",
            "Epoch[4/15], Step [410/469], Reconst Loss: 10674.2256, KL Div: 3239.1526\n",
            "Epoch[4/15], Step [420/469], Reconst Loss: 11210.0957, KL Div: 3131.8298\n",
            "Epoch[4/15], Step [430/469], Reconst Loss: 11119.9277, KL Div: 3129.4131\n",
            "Epoch[4/15], Step [440/469], Reconst Loss: 10955.9814, KL Div: 3262.6379\n",
            "Epoch[4/15], Step [450/469], Reconst Loss: 11136.0977, KL Div: 3116.5352\n",
            "Epoch[4/15], Step [460/469], Reconst Loss: 11020.6074, KL Div: 3226.9497\n",
            "Epoch[5/15], Step [10/469], Reconst Loss: 11158.1357, KL Div: 3057.1030\n",
            "Epoch[5/15], Step [20/469], Reconst Loss: 11115.0879, KL Div: 3299.6997\n",
            "Epoch[5/15], Step [30/469], Reconst Loss: 10785.0000, KL Div: 3171.7356\n",
            "Epoch[5/15], Step [40/469], Reconst Loss: 11220.4316, KL Div: 3123.7952\n",
            "Epoch[5/15], Step [50/469], Reconst Loss: 10903.7939, KL Div: 3208.0720\n",
            "Epoch[5/15], Step [60/469], Reconst Loss: 11430.2031, KL Div: 3220.5361\n",
            "Epoch[5/15], Step [70/469], Reconst Loss: 11084.9043, KL Div: 3205.8818\n",
            "Epoch[5/15], Step [80/469], Reconst Loss: 11330.1172, KL Div: 3131.5051\n",
            "Epoch[5/15], Step [90/469], Reconst Loss: 10881.8857, KL Div: 3072.2637\n",
            "Epoch[5/15], Step [100/469], Reconst Loss: 10556.9365, KL Div: 3154.3213\n",
            "Epoch[5/15], Step [110/469], Reconst Loss: 11152.5254, KL Div: 3211.5261\n",
            "Epoch[5/15], Step [120/469], Reconst Loss: 11165.7207, KL Div: 3157.9932\n",
            "Epoch[5/15], Step [130/469], Reconst Loss: 10908.3867, KL Div: 3188.8857\n",
            "Epoch[5/15], Step [140/469], Reconst Loss: 10917.1885, KL Div: 3150.8721\n",
            "Epoch[5/15], Step [150/469], Reconst Loss: 10884.5605, KL Div: 3018.5615\n",
            "Epoch[5/15], Step [160/469], Reconst Loss: 10786.3281, KL Div: 3082.7043\n",
            "Epoch[5/15], Step [170/469], Reconst Loss: 11213.1104, KL Div: 3190.1067\n",
            "Epoch[5/15], Step [180/469], Reconst Loss: 10678.1484, KL Div: 3103.5522\n",
            "Epoch[5/15], Step [190/469], Reconst Loss: 11172.8027, KL Div: 3311.0215\n",
            "Epoch[5/15], Step [200/469], Reconst Loss: 11327.6348, KL Div: 3199.4622\n",
            "Epoch[5/15], Step [210/469], Reconst Loss: 10614.4707, KL Div: 3060.1050\n",
            "Epoch[5/15], Step [220/469], Reconst Loss: 10453.5957, KL Div: 3103.3215\n",
            "Epoch[5/15], Step [230/469], Reconst Loss: 10908.1738, KL Div: 3193.5364\n",
            "Epoch[5/15], Step [240/469], Reconst Loss: 11182.4170, KL Div: 3268.9414\n",
            "Epoch[5/15], Step [250/469], Reconst Loss: 11069.4473, KL Div: 3158.1238\n",
            "Epoch[5/15], Step [260/469], Reconst Loss: 11267.1484, KL Div: 3146.7729\n",
            "Epoch[5/15], Step [270/469], Reconst Loss: 11002.6436, KL Div: 3177.3521\n",
            "Epoch[5/15], Step [280/469], Reconst Loss: 10972.4824, KL Div: 3111.4907\n",
            "Epoch[5/15], Step [290/469], Reconst Loss: 10490.0322, KL Div: 3169.5144\n",
            "Epoch[5/15], Step [300/469], Reconst Loss: 10717.7930, KL Div: 3226.1963\n",
            "Epoch[5/15], Step [310/469], Reconst Loss: 10846.4717, KL Div: 3117.0698\n",
            "Epoch[5/15], Step [320/469], Reconst Loss: 10567.2031, KL Div: 3159.3066\n",
            "Epoch[5/15], Step [330/469], Reconst Loss: 10686.2891, KL Div: 3223.5894\n",
            "Epoch[5/15], Step [340/469], Reconst Loss: 10383.1895, KL Div: 3076.1514\n",
            "Epoch[5/15], Step [350/469], Reconst Loss: 10641.8506, KL Div: 3167.0200\n",
            "Epoch[5/15], Step [360/469], Reconst Loss: 10847.6152, KL Div: 3151.5283\n",
            "Epoch[5/15], Step [370/469], Reconst Loss: 10906.1875, KL Div: 3185.9170\n",
            "Epoch[5/15], Step [380/469], Reconst Loss: 11035.8672, KL Div: 3207.2869\n",
            "Epoch[5/15], Step [390/469], Reconst Loss: 10792.9570, KL Div: 3099.2156\n",
            "Epoch[5/15], Step [400/469], Reconst Loss: 10739.2812, KL Div: 3180.6626\n",
            "Epoch[5/15], Step [410/469], Reconst Loss: 10889.8867, KL Div: 3187.6270\n",
            "Epoch[5/15], Step [420/469], Reconst Loss: 10855.0469, KL Div: 3265.9189\n",
            "Epoch[5/15], Step [430/469], Reconst Loss: 10606.4346, KL Div: 3040.2417\n",
            "Epoch[5/15], Step [440/469], Reconst Loss: 10934.6133, KL Div: 3172.7258\n",
            "Epoch[5/15], Step [450/469], Reconst Loss: 10649.1875, KL Div: 3113.7061\n",
            "Epoch[5/15], Step [460/469], Reconst Loss: 10642.3613, KL Div: 3241.1758\n",
            "Epoch[6/15], Step [10/469], Reconst Loss: 10893.4160, KL Div: 3288.0283\n",
            "Epoch[6/15], Step [20/469], Reconst Loss: 11132.7070, KL Div: 3147.3198\n",
            "Epoch[6/15], Step [30/469], Reconst Loss: 10530.6367, KL Div: 3236.7263\n",
            "Epoch[6/15], Step [40/469], Reconst Loss: 11107.9375, KL Div: 3210.1060\n",
            "Epoch[6/15], Step [50/469], Reconst Loss: 10783.8125, KL Div: 3238.4270\n",
            "Epoch[6/15], Step [60/469], Reconst Loss: 11096.3154, KL Div: 3192.6758\n",
            "Epoch[6/15], Step [70/469], Reconst Loss: 10401.6543, KL Div: 3165.2510\n",
            "Epoch[6/15], Step [80/469], Reconst Loss: 11022.3721, KL Div: 3185.0801\n",
            "Epoch[6/15], Step [90/469], Reconst Loss: 10653.0771, KL Div: 3059.1182\n",
            "Epoch[6/15], Step [100/469], Reconst Loss: 10730.7070, KL Div: 3353.3999\n",
            "Epoch[6/15], Step [110/469], Reconst Loss: 10999.8555, KL Div: 3173.3198\n",
            "Epoch[6/15], Step [120/469], Reconst Loss: 10778.3457, KL Div: 3110.3970\n",
            "Epoch[6/15], Step [130/469], Reconst Loss: 10425.9629, KL Div: 3191.8345\n",
            "Epoch[6/15], Step [140/469], Reconst Loss: 10874.9023, KL Div: 3147.3257\n",
            "Epoch[6/15], Step [150/469], Reconst Loss: 10435.0137, KL Div: 3047.4624\n",
            "Epoch[6/15], Step [160/469], Reconst Loss: 10500.7695, KL Div: 3298.6372\n",
            "Epoch[6/15], Step [170/469], Reconst Loss: 10844.4199, KL Div: 3082.9050\n",
            "Epoch[6/15], Step [180/469], Reconst Loss: 10989.5508, KL Div: 3300.8872\n",
            "Epoch[6/15], Step [190/469], Reconst Loss: 10547.5449, KL Div: 3241.0935\n",
            "Epoch[6/15], Step [200/469], Reconst Loss: 10762.2979, KL Div: 3226.2915\n",
            "Epoch[6/15], Step [210/469], Reconst Loss: 10692.2754, KL Div: 3169.7207\n",
            "Epoch[6/15], Step [220/469], Reconst Loss: 10754.2686, KL Div: 3220.7119\n",
            "Epoch[6/15], Step [230/469], Reconst Loss: 10680.4219, KL Div: 3115.4072\n",
            "Epoch[6/15], Step [240/469], Reconst Loss: 10996.8516, KL Div: 3259.1553\n",
            "Epoch[6/15], Step [250/469], Reconst Loss: 10932.8535, KL Div: 3189.7090\n",
            "Epoch[6/15], Step [260/469], Reconst Loss: 10652.9023, KL Div: 3173.3083\n",
            "Epoch[6/15], Step [270/469], Reconst Loss: 10698.1201, KL Div: 3231.7588\n",
            "Epoch[6/15], Step [280/469], Reconst Loss: 10536.1494, KL Div: 3134.2126\n",
            "Epoch[6/15], Step [290/469], Reconst Loss: 11096.3398, KL Div: 3240.4336\n",
            "Epoch[6/15], Step [300/469], Reconst Loss: 9975.9697, KL Div: 3118.8652\n",
            "Epoch[6/15], Step [310/469], Reconst Loss: 10752.1445, KL Div: 3298.4893\n",
            "Epoch[6/15], Step [320/469], Reconst Loss: 10682.7041, KL Div: 3129.4985\n",
            "Epoch[6/15], Step [330/469], Reconst Loss: 10644.2656, KL Div: 3218.3633\n",
            "Epoch[6/15], Step [340/469], Reconst Loss: 10942.8789, KL Div: 3141.0134\n",
            "Epoch[6/15], Step [350/469], Reconst Loss: 10004.1191, KL Div: 3095.0601\n",
            "Epoch[6/15], Step [360/469], Reconst Loss: 11139.8682, KL Div: 3330.1274\n",
            "Epoch[6/15], Step [370/469], Reconst Loss: 10763.6787, KL Div: 3107.9736\n",
            "Epoch[6/15], Step [380/469], Reconst Loss: 10681.2715, KL Div: 3280.0562\n",
            "Epoch[6/15], Step [390/469], Reconst Loss: 11130.3965, KL Div: 3245.6689\n",
            "Epoch[6/15], Step [400/469], Reconst Loss: 10972.3359, KL Div: 3196.3730\n",
            "Epoch[6/15], Step [410/469], Reconst Loss: 11288.7871, KL Div: 3203.0557\n",
            "Epoch[6/15], Step [420/469], Reconst Loss: 10786.0391, KL Div: 3212.4482\n",
            "Epoch[6/15], Step [430/469], Reconst Loss: 10797.7656, KL Div: 3177.9246\n",
            "Epoch[6/15], Step [440/469], Reconst Loss: 10303.2676, KL Div: 3308.2471\n",
            "Epoch[6/15], Step [450/469], Reconst Loss: 10452.6660, KL Div: 3254.4358\n",
            "Epoch[6/15], Step [460/469], Reconst Loss: 10171.4297, KL Div: 3046.8254\n",
            "Epoch[7/15], Step [10/469], Reconst Loss: 10433.7539, KL Div: 3283.7065\n",
            "Epoch[7/15], Step [20/469], Reconst Loss: 10362.7383, KL Div: 3152.5581\n",
            "Epoch[7/15], Step [30/469], Reconst Loss: 10387.0449, KL Div: 3208.2051\n",
            "Epoch[7/15], Step [40/469], Reconst Loss: 10971.8096, KL Div: 3095.6750\n",
            "Epoch[7/15], Step [50/469], Reconst Loss: 10776.9971, KL Div: 3357.1160\n",
            "Epoch[7/15], Step [60/469], Reconst Loss: 10678.0342, KL Div: 3204.6790\n",
            "Epoch[7/15], Step [70/469], Reconst Loss: 9943.2578, KL Div: 3092.1562\n",
            "Epoch[7/15], Step [80/469], Reconst Loss: 10199.6826, KL Div: 3180.7722\n",
            "Epoch[7/15], Step [90/469], Reconst Loss: 10801.9912, KL Div: 3183.5747\n",
            "Epoch[7/15], Step [100/469], Reconst Loss: 10294.1816, KL Div: 3242.5508\n",
            "Epoch[7/15], Step [110/469], Reconst Loss: 10765.8711, KL Div: 3210.7744\n",
            "Epoch[7/15], Step [120/469], Reconst Loss: 10546.7451, KL Div: 3146.2852\n",
            "Epoch[7/15], Step [130/469], Reconst Loss: 10428.2090, KL Div: 3266.1001\n",
            "Epoch[7/15], Step [140/469], Reconst Loss: 10742.3086, KL Div: 3105.4746\n",
            "Epoch[7/15], Step [150/469], Reconst Loss: 10588.6855, KL Div: 3295.8022\n",
            "Epoch[7/15], Step [160/469], Reconst Loss: 10709.5537, KL Div: 3270.5281\n",
            "Epoch[7/15], Step [170/469], Reconst Loss: 10849.1055, KL Div: 3203.7300\n",
            "Epoch[7/15], Step [180/469], Reconst Loss: 10646.0859, KL Div: 3161.3477\n",
            "Epoch[7/15], Step [190/469], Reconst Loss: 10455.1367, KL Div: 3170.3906\n",
            "Epoch[7/15], Step [200/469], Reconst Loss: 10025.1787, KL Div: 3106.3755\n",
            "Epoch[7/15], Step [210/469], Reconst Loss: 10536.1699, KL Div: 3069.0068\n",
            "Epoch[7/15], Step [220/469], Reconst Loss: 10337.9141, KL Div: 3137.8538\n",
            "Epoch[7/15], Step [230/469], Reconst Loss: 10489.4316, KL Div: 3308.4263\n",
            "Epoch[7/15], Step [240/469], Reconst Loss: 10452.0957, KL Div: 3102.8762\n",
            "Epoch[7/15], Step [250/469], Reconst Loss: 10996.0723, KL Div: 3179.5649\n",
            "Epoch[7/15], Step [260/469], Reconst Loss: 9952.5918, KL Div: 3092.8069\n",
            "Epoch[7/15], Step [270/469], Reconst Loss: 10738.6465, KL Div: 3129.0371\n",
            "Epoch[7/15], Step [280/469], Reconst Loss: 10839.5586, KL Div: 3232.6726\n",
            "Epoch[7/15], Step [290/469], Reconst Loss: 10336.4717, KL Div: 3234.6348\n",
            "Epoch[7/15], Step [300/469], Reconst Loss: 10741.1543, KL Div: 3225.4746\n",
            "Epoch[7/15], Step [310/469], Reconst Loss: 10406.1211, KL Div: 3241.7822\n",
            "Epoch[7/15], Step [320/469], Reconst Loss: 10388.6953, KL Div: 3094.8442\n",
            "Epoch[7/15], Step [330/469], Reconst Loss: 10142.4551, KL Div: 3147.5857\n",
            "Epoch[7/15], Step [340/469], Reconst Loss: 10992.5820, KL Div: 3106.4868\n",
            "Epoch[7/15], Step [350/469], Reconst Loss: 10412.6152, KL Div: 3190.1279\n",
            "Epoch[7/15], Step [360/469], Reconst Loss: 11145.8789, KL Div: 3262.9673\n",
            "Epoch[7/15], Step [370/469], Reconst Loss: 10238.5234, KL Div: 3212.4150\n",
            "Epoch[7/15], Step [380/469], Reconst Loss: 10935.9375, KL Div: 3225.7251\n",
            "Epoch[7/15], Step [390/469], Reconst Loss: 10549.8281, KL Div: 3216.5835\n",
            "Epoch[7/15], Step [400/469], Reconst Loss: 10663.3809, KL Div: 3244.9653\n",
            "Epoch[7/15], Step [410/469], Reconst Loss: 10443.6543, KL Div: 3157.4800\n",
            "Epoch[7/15], Step [420/469], Reconst Loss: 9933.1777, KL Div: 3182.9414\n",
            "Epoch[7/15], Step [430/469], Reconst Loss: 10583.4746, KL Div: 3301.8625\n",
            "Epoch[7/15], Step [440/469], Reconst Loss: 10653.8662, KL Div: 3215.6567\n",
            "Epoch[7/15], Step [450/469], Reconst Loss: 10507.9570, KL Div: 3159.9458\n",
            "Epoch[7/15], Step [460/469], Reconst Loss: 10465.1094, KL Div: 3385.7190\n",
            "Epoch[8/15], Step [10/469], Reconst Loss: 10408.5146, KL Div: 3203.6953\n",
            "Epoch[8/15], Step [20/469], Reconst Loss: 10415.6963, KL Div: 3212.4082\n",
            "Epoch[8/15], Step [30/469], Reconst Loss: 10352.0225, KL Div: 3150.8225\n",
            "Epoch[8/15], Step [40/469], Reconst Loss: 10374.9258, KL Div: 3161.2588\n",
            "Epoch[8/15], Step [50/469], Reconst Loss: 10536.2754, KL Div: 3220.5034\n",
            "Epoch[8/15], Step [60/469], Reconst Loss: 10620.9287, KL Div: 3324.6089\n",
            "Epoch[8/15], Step [70/469], Reconst Loss: 10496.7910, KL Div: 3171.4839\n",
            "Epoch[8/15], Step [80/469], Reconst Loss: 9793.7578, KL Div: 3140.6860\n",
            "Epoch[8/15], Step [90/469], Reconst Loss: 10636.7354, KL Div: 3284.9980\n",
            "Epoch[8/15], Step [100/469], Reconst Loss: 10064.7148, KL Div: 3202.3354\n",
            "Epoch[8/15], Step [110/469], Reconst Loss: 10463.9746, KL Div: 3134.7319\n",
            "Epoch[8/15], Step [120/469], Reconst Loss: 10329.6953, KL Div: 3160.5479\n",
            "Epoch[8/15], Step [130/469], Reconst Loss: 10385.9170, KL Div: 3171.9749\n",
            "Epoch[8/15], Step [140/469], Reconst Loss: 10667.1543, KL Div: 3171.7727\n",
            "Epoch[8/15], Step [150/469], Reconst Loss: 9956.2188, KL Div: 3212.8301\n",
            "Epoch[8/15], Step [160/469], Reconst Loss: 10590.4473, KL Div: 3122.7529\n",
            "Epoch[8/15], Step [170/469], Reconst Loss: 10684.8242, KL Div: 3209.5342\n",
            "Epoch[8/15], Step [180/469], Reconst Loss: 10726.8301, KL Div: 3263.5991\n",
            "Epoch[8/15], Step [190/469], Reconst Loss: 10734.5527, KL Div: 3183.5269\n",
            "Epoch[8/15], Step [200/469], Reconst Loss: 11001.1211, KL Div: 3353.3262\n",
            "Epoch[8/15], Step [210/469], Reconst Loss: 10428.8320, KL Div: 3256.9985\n",
            "Epoch[8/15], Step [220/469], Reconst Loss: 10193.2656, KL Div: 3136.9541\n",
            "Epoch[8/15], Step [230/469], Reconst Loss: 10687.4160, KL Div: 3279.4434\n",
            "Epoch[8/15], Step [240/469], Reconst Loss: 10417.9414, KL Div: 3169.2559\n",
            "Epoch[8/15], Step [250/469], Reconst Loss: 10494.2402, KL Div: 3173.6670\n",
            "Epoch[8/15], Step [260/469], Reconst Loss: 10609.9395, KL Div: 3069.9849\n",
            "Epoch[8/15], Step [270/469], Reconst Loss: 10638.5605, KL Div: 3235.9893\n",
            "Epoch[8/15], Step [280/469], Reconst Loss: 10507.9775, KL Div: 3224.6465\n",
            "Epoch[8/15], Step [290/469], Reconst Loss: 10673.2285, KL Div: 3200.0176\n",
            "Epoch[8/15], Step [300/469], Reconst Loss: 10297.0078, KL Div: 3241.5837\n",
            "Epoch[8/15], Step [310/469], Reconst Loss: 10467.4902, KL Div: 3239.4526\n",
            "Epoch[8/15], Step [320/469], Reconst Loss: 10433.4395, KL Div: 3227.4785\n",
            "Epoch[8/15], Step [330/469], Reconst Loss: 10353.0742, KL Div: 3123.9209\n",
            "Epoch[8/15], Step [340/469], Reconst Loss: 10185.3691, KL Div: 3181.2720\n",
            "Epoch[8/15], Step [350/469], Reconst Loss: 10162.0273, KL Div: 3129.2329\n",
            "Epoch[8/15], Step [360/469], Reconst Loss: 10355.9922, KL Div: 3204.3257\n",
            "Epoch[8/15], Step [370/469], Reconst Loss: 10375.5273, KL Div: 3182.2039\n",
            "Epoch[8/15], Step [380/469], Reconst Loss: 10027.1055, KL Div: 3226.5679\n",
            "Epoch[8/15], Step [390/469], Reconst Loss: 10175.1924, KL Div: 3261.7046\n",
            "Epoch[8/15], Step [400/469], Reconst Loss: 10325.3672, KL Div: 3226.2717\n",
            "Epoch[8/15], Step [410/469], Reconst Loss: 10450.0312, KL Div: 3287.7393\n",
            "Epoch[8/15], Step [420/469], Reconst Loss: 10420.2188, KL Div: 3131.7363\n",
            "Epoch[8/15], Step [430/469], Reconst Loss: 10807.7207, KL Div: 3313.6045\n",
            "Epoch[8/15], Step [440/469], Reconst Loss: 10453.4795, KL Div: 3089.0935\n",
            "Epoch[8/15], Step [450/469], Reconst Loss: 10008.6553, KL Div: 3282.3291\n",
            "Epoch[8/15], Step [460/469], Reconst Loss: 10799.1074, KL Div: 3309.4453\n",
            "Epoch[9/15], Step [10/469], Reconst Loss: 10219.4131, KL Div: 3292.4922\n",
            "Epoch[9/15], Step [20/469], Reconst Loss: 10604.3945, KL Div: 3216.3896\n",
            "Epoch[9/15], Step [30/469], Reconst Loss: 10222.9629, KL Div: 3193.0298\n",
            "Epoch[9/15], Step [40/469], Reconst Loss: 10332.0000, KL Div: 3113.4111\n",
            "Epoch[9/15], Step [50/469], Reconst Loss: 10554.6982, KL Div: 3331.5630\n",
            "Epoch[9/15], Step [60/469], Reconst Loss: 10264.0908, KL Div: 3165.1709\n",
            "Epoch[9/15], Step [70/469], Reconst Loss: 10075.4180, KL Div: 3134.6958\n",
            "Epoch[9/15], Step [80/469], Reconst Loss: 10328.6543, KL Div: 3271.7856\n",
            "Epoch[9/15], Step [90/469], Reconst Loss: 10539.3174, KL Div: 3261.9685\n",
            "Epoch[9/15], Step [100/469], Reconst Loss: 10624.8945, KL Div: 3270.6602\n",
            "Epoch[9/15], Step [110/469], Reconst Loss: 9796.5557, KL Div: 3082.9678\n",
            "Epoch[9/15], Step [120/469], Reconst Loss: 10117.4219, KL Div: 3249.5493\n",
            "Epoch[9/15], Step [130/469], Reconst Loss: 10562.5742, KL Div: 3205.5757\n",
            "Epoch[9/15], Step [140/469], Reconst Loss: 10199.7168, KL Div: 3118.8003\n",
            "Epoch[9/15], Step [150/469], Reconst Loss: 9937.0352, KL Div: 3177.8428\n",
            "Epoch[9/15], Step [160/469], Reconst Loss: 10530.7256, KL Div: 3288.5469\n",
            "Epoch[9/15], Step [170/469], Reconst Loss: 9998.1055, KL Div: 3233.5916\n",
            "Epoch[9/15], Step [180/469], Reconst Loss: 10422.0830, KL Div: 3250.8411\n",
            "Epoch[9/15], Step [190/469], Reconst Loss: 10278.5215, KL Div: 3069.0894\n",
            "Epoch[9/15], Step [200/469], Reconst Loss: 10253.5996, KL Div: 3306.6895\n",
            "Epoch[9/15], Step [210/469], Reconst Loss: 10270.8887, KL Div: 3177.9229\n",
            "Epoch[9/15], Step [220/469], Reconst Loss: 10473.8574, KL Div: 3181.8220\n",
            "Epoch[9/15], Step [230/469], Reconst Loss: 10500.5850, KL Div: 3189.3555\n",
            "Epoch[9/15], Step [240/469], Reconst Loss: 10637.1582, KL Div: 3212.3789\n",
            "Epoch[9/15], Step [250/469], Reconst Loss: 10525.7617, KL Div: 3232.4609\n",
            "Epoch[9/15], Step [260/469], Reconst Loss: 10518.0273, KL Div: 3297.0566\n",
            "Epoch[9/15], Step [270/469], Reconst Loss: 10350.8477, KL Div: 3194.2256\n",
            "Epoch[9/15], Step [280/469], Reconst Loss: 10787.1133, KL Div: 3227.3003\n",
            "Epoch[9/15], Step [290/469], Reconst Loss: 10482.7852, KL Div: 3193.6479\n",
            "Epoch[9/15], Step [300/469], Reconst Loss: 10440.9717, KL Div: 3285.3586\n",
            "Epoch[9/15], Step [310/469], Reconst Loss: 10243.6348, KL Div: 3309.9209\n",
            "Epoch[9/15], Step [320/469], Reconst Loss: 10452.5186, KL Div: 3228.1492\n",
            "Epoch[9/15], Step [330/469], Reconst Loss: 10829.1709, KL Div: 3191.7375\n",
            "Epoch[9/15], Step [340/469], Reconst Loss: 9736.9668, KL Div: 3150.5798\n",
            "Epoch[9/15], Step [350/469], Reconst Loss: 10057.9004, KL Div: 3147.0542\n",
            "Epoch[9/15], Step [360/469], Reconst Loss: 10556.1074, KL Div: 3207.1936\n",
            "Epoch[9/15], Step [370/469], Reconst Loss: 10491.9375, KL Div: 3281.0259\n",
            "Epoch[9/15], Step [380/469], Reconst Loss: 9900.7012, KL Div: 3114.8154\n",
            "Epoch[9/15], Step [390/469], Reconst Loss: 10659.4922, KL Div: 3270.1702\n",
            "Epoch[9/15], Step [400/469], Reconst Loss: 10543.4141, KL Div: 3114.2009\n",
            "Epoch[9/15], Step [410/469], Reconst Loss: 10051.0664, KL Div: 3139.9517\n",
            "Epoch[9/15], Step [420/469], Reconst Loss: 10035.3525, KL Div: 3086.9260\n",
            "Epoch[9/15], Step [430/469], Reconst Loss: 10500.0664, KL Div: 3140.1887\n",
            "Epoch[9/15], Step [440/469], Reconst Loss: 10171.8135, KL Div: 3331.7676\n",
            "Epoch[9/15], Step [450/469], Reconst Loss: 10267.5293, KL Div: 3100.5964\n",
            "Epoch[9/15], Step [460/469], Reconst Loss: 10474.7168, KL Div: 3233.0398\n",
            "Epoch[10/15], Step [10/469], Reconst Loss: 10460.3857, KL Div: 3239.6392\n",
            "Epoch[10/15], Step [20/469], Reconst Loss: 9799.1113, KL Div: 3209.1313\n",
            "Epoch[10/15], Step [30/469], Reconst Loss: 10171.8193, KL Div: 3296.5977\n",
            "Epoch[10/15], Step [40/469], Reconst Loss: 10207.4473, KL Div: 3203.9368\n",
            "Epoch[10/15], Step [50/469], Reconst Loss: 10427.9639, KL Div: 3175.1274\n",
            "Epoch[10/15], Step [60/469], Reconst Loss: 10536.3076, KL Div: 3212.5967\n",
            "Epoch[10/15], Step [70/469], Reconst Loss: 10210.3330, KL Div: 3303.4873\n",
            "Epoch[10/15], Step [80/469], Reconst Loss: 10295.7812, KL Div: 3180.6206\n",
            "Epoch[10/15], Step [90/469], Reconst Loss: 10332.5098, KL Div: 3222.8491\n",
            "Epoch[10/15], Step [100/469], Reconst Loss: 10286.0332, KL Div: 3169.1709\n",
            "Epoch[10/15], Step [110/469], Reconst Loss: 10646.9131, KL Div: 3265.7197\n",
            "Epoch[10/15], Step [120/469], Reconst Loss: 10334.8867, KL Div: 3313.6084\n",
            "Epoch[10/15], Step [130/469], Reconst Loss: 10252.4980, KL Div: 3180.0935\n",
            "Epoch[10/15], Step [140/469], Reconst Loss: 10323.6426, KL Div: 3360.9087\n",
            "Epoch[10/15], Step [150/469], Reconst Loss: 10202.2471, KL Div: 3173.2144\n",
            "Epoch[10/15], Step [160/469], Reconst Loss: 10902.5322, KL Div: 3314.8130\n",
            "Epoch[10/15], Step [170/469], Reconst Loss: 10261.1406, KL Div: 3116.0583\n",
            "Epoch[10/15], Step [180/469], Reconst Loss: 10498.1055, KL Div: 3194.9722\n",
            "Epoch[10/15], Step [190/469], Reconst Loss: 10550.7100, KL Div: 3249.2100\n",
            "Epoch[10/15], Step [200/469], Reconst Loss: 10095.7754, KL Div: 3250.0405\n",
            "Epoch[10/15], Step [210/469], Reconst Loss: 9924.0234, KL Div: 3127.8853\n",
            "Epoch[10/15], Step [220/469], Reconst Loss: 10232.0078, KL Div: 3126.6619\n",
            "Epoch[10/15], Step [230/469], Reconst Loss: 10669.8154, KL Div: 3276.7571\n",
            "Epoch[10/15], Step [240/469], Reconst Loss: 10597.7686, KL Div: 3261.5552\n",
            "Epoch[10/15], Step [250/469], Reconst Loss: 10876.9902, KL Div: 3270.3044\n",
            "Epoch[10/15], Step [260/469], Reconst Loss: 10628.3203, KL Div: 3235.5303\n",
            "Epoch[10/15], Step [270/469], Reconst Loss: 10653.9580, KL Div: 3249.2322\n",
            "Epoch[10/15], Step [280/469], Reconst Loss: 10192.9395, KL Div: 3239.6897\n",
            "Epoch[10/15], Step [290/469], Reconst Loss: 10275.2773, KL Div: 3128.9985\n",
            "Epoch[10/15], Step [300/469], Reconst Loss: 10657.5176, KL Div: 3323.4553\n",
            "Epoch[10/15], Step [310/469], Reconst Loss: 10558.7148, KL Div: 3217.7039\n",
            "Epoch[10/15], Step [320/469], Reconst Loss: 10362.9873, KL Div: 3245.1306\n",
            "Epoch[10/15], Step [330/469], Reconst Loss: 10205.4951, KL Div: 3171.2241\n",
            "Epoch[10/15], Step [340/469], Reconst Loss: 10075.9316, KL Div: 3184.1890\n",
            "Epoch[10/15], Step [350/469], Reconst Loss: 10157.6016, KL Div: 3135.8066\n",
            "Epoch[10/15], Step [360/469], Reconst Loss: 10461.5098, KL Div: 3238.9382\n",
            "Epoch[10/15], Step [370/469], Reconst Loss: 10973.6934, KL Div: 3333.4521\n",
            "Epoch[10/15], Step [380/469], Reconst Loss: 10303.5020, KL Div: 3207.7188\n",
            "Epoch[10/15], Step [390/469], Reconst Loss: 10367.5254, KL Div: 3200.0225\n",
            "Epoch[10/15], Step [400/469], Reconst Loss: 10577.0918, KL Div: 3343.5205\n",
            "Epoch[10/15], Step [410/469], Reconst Loss: 10286.2324, KL Div: 3160.9216\n",
            "Epoch[10/15], Step [420/469], Reconst Loss: 9905.2754, KL Div: 3176.8008\n",
            "Epoch[10/15], Step [430/469], Reconst Loss: 9838.0684, KL Div: 3170.8701\n",
            "Epoch[10/15], Step [440/469], Reconst Loss: 10728.9355, KL Div: 3174.6914\n",
            "Epoch[10/15], Step [450/469], Reconst Loss: 10570.0791, KL Div: 3291.5935\n",
            "Epoch[10/15], Step [460/469], Reconst Loss: 9987.7070, KL Div: 3135.3838\n",
            "Epoch[11/15], Step [10/469], Reconst Loss: 10598.8037, KL Div: 3270.2925\n",
            "Epoch[11/15], Step [20/469], Reconst Loss: 10298.3975, KL Div: 3250.0540\n",
            "Epoch[11/15], Step [30/469], Reconst Loss: 10939.3398, KL Div: 3310.9746\n",
            "Epoch[11/15], Step [40/469], Reconst Loss: 10581.9326, KL Div: 3361.9104\n",
            "Epoch[11/15], Step [50/469], Reconst Loss: 10569.6152, KL Div: 3236.5105\n",
            "Epoch[11/15], Step [60/469], Reconst Loss: 10246.5020, KL Div: 3257.0908\n",
            "Epoch[11/15], Step [70/469], Reconst Loss: 9860.2480, KL Div: 3151.1069\n",
            "Epoch[11/15], Step [80/469], Reconst Loss: 9756.8281, KL Div: 3120.7290\n",
            "Epoch[11/15], Step [90/469], Reconst Loss: 10388.5791, KL Div: 3282.2026\n",
            "Epoch[11/15], Step [100/469], Reconst Loss: 10368.0459, KL Div: 3269.4690\n",
            "Epoch[11/15], Step [110/469], Reconst Loss: 10277.3027, KL Div: 3196.7427\n",
            "Epoch[11/15], Step [120/469], Reconst Loss: 9959.0195, KL Div: 3162.4780\n",
            "Epoch[11/15], Step [130/469], Reconst Loss: 10000.9609, KL Div: 3290.7378\n",
            "Epoch[11/15], Step [140/469], Reconst Loss: 10358.2559, KL Div: 3292.9893\n",
            "Epoch[11/15], Step [150/469], Reconst Loss: 10174.0361, KL Div: 3227.1133\n",
            "Epoch[11/15], Step [160/469], Reconst Loss: 10026.3145, KL Div: 3249.9092\n",
            "Epoch[11/15], Step [170/469], Reconst Loss: 10630.4570, KL Div: 3282.3765\n",
            "Epoch[11/15], Step [180/469], Reconst Loss: 10372.1572, KL Div: 3253.0137\n",
            "Epoch[11/15], Step [190/469], Reconst Loss: 9976.6797, KL Div: 3203.6372\n",
            "Epoch[11/15], Step [200/469], Reconst Loss: 10193.0586, KL Div: 3245.3701\n",
            "Epoch[11/15], Step [210/469], Reconst Loss: 10574.9863, KL Div: 3322.4348\n",
            "Epoch[11/15], Step [220/469], Reconst Loss: 10378.2148, KL Div: 3226.6201\n",
            "Epoch[11/15], Step [230/469], Reconst Loss: 9884.4150, KL Div: 3267.2720\n",
            "Epoch[11/15], Step [240/469], Reconst Loss: 10352.4199, KL Div: 3179.5674\n",
            "Epoch[11/15], Step [250/469], Reconst Loss: 10650.3574, KL Div: 3305.7390\n",
            "Epoch[11/15], Step [260/469], Reconst Loss: 10223.6523, KL Div: 3217.2759\n",
            "Epoch[11/15], Step [270/469], Reconst Loss: 10140.7441, KL Div: 3256.3840\n",
            "Epoch[11/15], Step [280/469], Reconst Loss: 10527.2480, KL Div: 3280.0767\n",
            "Epoch[11/15], Step [290/469], Reconst Loss: 10154.3916, KL Div: 3256.7654\n",
            "Epoch[11/15], Step [300/469], Reconst Loss: 10152.9766, KL Div: 3185.0649\n",
            "Epoch[11/15], Step [310/469], Reconst Loss: 10861.0762, KL Div: 3183.3818\n",
            "Epoch[11/15], Step [320/469], Reconst Loss: 10001.9678, KL Div: 3180.3516\n",
            "Epoch[11/15], Step [330/469], Reconst Loss: 10378.8457, KL Div: 3230.2500\n",
            "Epoch[11/15], Step [340/469], Reconst Loss: 10730.8506, KL Div: 3290.7439\n",
            "Epoch[11/15], Step [350/469], Reconst Loss: 10667.4883, KL Div: 3332.3452\n",
            "Epoch[11/15], Step [360/469], Reconst Loss: 10306.1797, KL Div: 3206.6292\n",
            "Epoch[11/15], Step [370/469], Reconst Loss: 10087.8516, KL Div: 3226.8464\n",
            "Epoch[11/15], Step [380/469], Reconst Loss: 9906.9941, KL Div: 3251.9004\n",
            "Epoch[11/15], Step [390/469], Reconst Loss: 10198.4697, KL Div: 3247.9932\n",
            "Epoch[11/15], Step [400/469], Reconst Loss: 9888.2129, KL Div: 3245.0078\n",
            "Epoch[11/15], Step [410/469], Reconst Loss: 10564.2334, KL Div: 3301.9580\n",
            "Epoch[11/15], Step [420/469], Reconst Loss: 10741.6318, KL Div: 3279.6558\n",
            "Epoch[11/15], Step [430/469], Reconst Loss: 9977.4561, KL Div: 3164.0237\n",
            "Epoch[11/15], Step [440/469], Reconst Loss: 10158.8164, KL Div: 3232.7139\n",
            "Epoch[11/15], Step [450/469], Reconst Loss: 9852.3799, KL Div: 3178.1614\n",
            "Epoch[11/15], Step [460/469], Reconst Loss: 10027.5596, KL Div: 3125.9087\n",
            "Epoch[12/15], Step [10/469], Reconst Loss: 10216.2363, KL Div: 3313.2483\n",
            "Epoch[12/15], Step [20/469], Reconst Loss: 10154.7461, KL Div: 3244.1438\n",
            "Epoch[12/15], Step [30/469], Reconst Loss: 10055.6934, KL Div: 3143.4243\n",
            "Epoch[12/15], Step [40/469], Reconst Loss: 10399.9688, KL Div: 3240.5747\n",
            "Epoch[12/15], Step [50/469], Reconst Loss: 10304.0156, KL Div: 3225.1655\n",
            "Epoch[12/15], Step [60/469], Reconst Loss: 10478.2266, KL Div: 3331.1533\n",
            "Epoch[12/15], Step [70/469], Reconst Loss: 10732.9902, KL Div: 3290.0859\n",
            "Epoch[12/15], Step [80/469], Reconst Loss: 10443.3984, KL Div: 3284.1316\n",
            "Epoch[12/15], Step [90/469], Reconst Loss: 10107.4277, KL Div: 3164.7473\n",
            "Epoch[12/15], Step [100/469], Reconst Loss: 10372.1455, KL Div: 3254.7324\n",
            "Epoch[12/15], Step [110/469], Reconst Loss: 10463.2363, KL Div: 3211.3950\n",
            "Epoch[12/15], Step [120/469], Reconst Loss: 9867.5176, KL Div: 3208.9292\n",
            "Epoch[12/15], Step [130/469], Reconst Loss: 10083.6416, KL Div: 3159.6235\n",
            "Epoch[12/15], Step [140/469], Reconst Loss: 10229.7480, KL Div: 3210.9463\n",
            "Epoch[12/15], Step [150/469], Reconst Loss: 10072.3223, KL Div: 3120.0464\n",
            "Epoch[12/15], Step [160/469], Reconst Loss: 9866.1602, KL Div: 3300.3494\n",
            "Epoch[12/15], Step [170/469], Reconst Loss: 10744.0654, KL Div: 3266.8152\n",
            "Epoch[12/15], Step [180/469], Reconst Loss: 10110.6084, KL Div: 3261.4617\n",
            "Epoch[12/15], Step [190/469], Reconst Loss: 10302.3770, KL Div: 3252.1401\n",
            "Epoch[12/15], Step [200/469], Reconst Loss: 10128.2383, KL Div: 3232.1648\n",
            "Epoch[12/15], Step [210/469], Reconst Loss: 10243.8750, KL Div: 3293.8438\n",
            "Epoch[12/15], Step [220/469], Reconst Loss: 10743.6016, KL Div: 3326.0376\n",
            "Epoch[12/15], Step [230/469], Reconst Loss: 10367.6602, KL Div: 3350.9102\n",
            "Epoch[12/15], Step [240/469], Reconst Loss: 10098.6084, KL Div: 3271.2976\n",
            "Epoch[12/15], Step [250/469], Reconst Loss: 10440.6465, KL Div: 3172.0132\n",
            "Epoch[12/15], Step [260/469], Reconst Loss: 10016.4863, KL Div: 3187.8486\n",
            "Epoch[12/15], Step [270/469], Reconst Loss: 10029.8740, KL Div: 3308.8862\n",
            "Epoch[12/15], Step [280/469], Reconst Loss: 10241.3604, KL Div: 3237.2549\n",
            "Epoch[12/15], Step [290/469], Reconst Loss: 10027.3555, KL Div: 3177.8972\n",
            "Epoch[12/15], Step [300/469], Reconst Loss: 10234.0859, KL Div: 3226.9023\n",
            "Epoch[12/15], Step [310/469], Reconst Loss: 10736.2539, KL Div: 3343.7759\n",
            "Epoch[12/15], Step [320/469], Reconst Loss: 10256.9941, KL Div: 3179.0447\n",
            "Epoch[12/15], Step [330/469], Reconst Loss: 10310.5068, KL Div: 3239.1162\n",
            "Epoch[12/15], Step [340/469], Reconst Loss: 10597.3359, KL Div: 3275.4834\n",
            "Epoch[12/15], Step [350/469], Reconst Loss: 10018.7344, KL Div: 3137.9121\n",
            "Epoch[12/15], Step [360/469], Reconst Loss: 10169.3809, KL Div: 3236.3989\n",
            "Epoch[12/15], Step [370/469], Reconst Loss: 10207.9189, KL Div: 3267.5923\n",
            "Epoch[12/15], Step [380/469], Reconst Loss: 9973.9141, KL Div: 3248.0112\n",
            "Epoch[12/15], Step [390/469], Reconst Loss: 10545.9395, KL Div: 3254.2427\n",
            "Epoch[12/15], Step [400/469], Reconst Loss: 10064.1797, KL Div: 3272.7007\n",
            "Epoch[12/15], Step [410/469], Reconst Loss: 10549.3789, KL Div: 3249.0371\n",
            "Epoch[12/15], Step [420/469], Reconst Loss: 10521.7109, KL Div: 3181.5977\n",
            "Epoch[12/15], Step [430/469], Reconst Loss: 10158.9219, KL Div: 3305.4932\n",
            "Epoch[12/15], Step [440/469], Reconst Loss: 10122.0918, KL Div: 3162.5828\n",
            "Epoch[12/15], Step [450/469], Reconst Loss: 10459.2529, KL Div: 3251.5898\n",
            "Epoch[12/15], Step [460/469], Reconst Loss: 10221.3320, KL Div: 3296.0042\n",
            "Epoch[13/15], Step [10/469], Reconst Loss: 10038.2715, KL Div: 3282.6685\n",
            "Epoch[13/15], Step [20/469], Reconst Loss: 10491.2422, KL Div: 3254.2175\n",
            "Epoch[13/15], Step [30/469], Reconst Loss: 10660.2148, KL Div: 3283.5532\n",
            "Epoch[13/15], Step [40/469], Reconst Loss: 9913.4531, KL Div: 3191.8013\n",
            "Epoch[13/15], Step [50/469], Reconst Loss: 10331.6777, KL Div: 3198.2097\n",
            "Epoch[13/15], Step [60/469], Reconst Loss: 10469.5645, KL Div: 3288.7041\n",
            "Epoch[13/15], Step [70/469], Reconst Loss: 9835.2129, KL Div: 3229.7485\n",
            "Epoch[13/15], Step [80/469], Reconst Loss: 10036.6436, KL Div: 3239.7434\n",
            "Epoch[13/15], Step [90/469], Reconst Loss: 10106.6924, KL Div: 3192.7793\n",
            "Epoch[13/15], Step [100/469], Reconst Loss: 9969.8359, KL Div: 3165.6655\n",
            "Epoch[13/15], Step [110/469], Reconst Loss: 10415.4648, KL Div: 3368.0420\n",
            "Epoch[13/15], Step [120/469], Reconst Loss: 10223.2812, KL Div: 3228.8071\n",
            "Epoch[13/15], Step [130/469], Reconst Loss: 10419.5410, KL Div: 3190.7900\n",
            "Epoch[13/15], Step [140/469], Reconst Loss: 10159.3262, KL Div: 3175.1094\n",
            "Epoch[13/15], Step [150/469], Reconst Loss: 10231.7646, KL Div: 3261.6821\n",
            "Epoch[13/15], Step [160/469], Reconst Loss: 10178.8604, KL Div: 3197.4241\n",
            "Epoch[13/15], Step [170/469], Reconst Loss: 9989.0049, KL Div: 3212.1023\n",
            "Epoch[13/15], Step [180/469], Reconst Loss: 10130.0654, KL Div: 3215.8608\n",
            "Epoch[13/15], Step [190/469], Reconst Loss: 10011.6953, KL Div: 3157.4226\n",
            "Epoch[13/15], Step [200/469], Reconst Loss: 10316.7285, KL Div: 3211.5288\n",
            "Epoch[13/15], Step [210/469], Reconst Loss: 9340.7754, KL Div: 3026.8491\n",
            "Epoch[13/15], Step [220/469], Reconst Loss: 9717.6465, KL Div: 3287.2104\n",
            "Epoch[13/15], Step [230/469], Reconst Loss: 9968.6328, KL Div: 3166.3687\n",
            "Epoch[13/15], Step [240/469], Reconst Loss: 9847.1377, KL Div: 3248.6733\n",
            "Epoch[13/15], Step [250/469], Reconst Loss: 10348.4453, KL Div: 3218.2952\n",
            "Epoch[13/15], Step [260/469], Reconst Loss: 10235.5840, KL Div: 3278.7649\n",
            "Epoch[13/15], Step [270/469], Reconst Loss: 9921.4990, KL Div: 3149.8613\n",
            "Epoch[13/15], Step [280/469], Reconst Loss: 10406.6250, KL Div: 3250.6123\n",
            "Epoch[13/15], Step [290/469], Reconst Loss: 10021.2227, KL Div: 3298.9795\n",
            "Epoch[13/15], Step [300/469], Reconst Loss: 10703.1426, KL Div: 3203.9014\n",
            "Epoch[13/15], Step [310/469], Reconst Loss: 10061.9160, KL Div: 3265.9116\n",
            "Epoch[13/15], Step [320/469], Reconst Loss: 9890.0566, KL Div: 3187.8940\n",
            "Epoch[13/15], Step [330/469], Reconst Loss: 10273.9336, KL Div: 3273.3013\n",
            "Epoch[13/15], Step [340/469], Reconst Loss: 10471.8652, KL Div: 3183.5820\n",
            "Epoch[13/15], Step [350/469], Reconst Loss: 9711.5205, KL Div: 3110.9854\n",
            "Epoch[13/15], Step [360/469], Reconst Loss: 10204.9082, KL Div: 3290.3943\n",
            "Epoch[13/15], Step [370/469], Reconst Loss: 10060.9473, KL Div: 3074.9619\n",
            "Epoch[13/15], Step [380/469], Reconst Loss: 10057.1211, KL Div: 3166.1387\n",
            "Epoch[13/15], Step [390/469], Reconst Loss: 10332.5010, KL Div: 3229.0078\n",
            "Epoch[13/15], Step [400/469], Reconst Loss: 10375.3145, KL Div: 3171.8247\n",
            "Epoch[13/15], Step [410/469], Reconst Loss: 10517.6543, KL Div: 3245.7153\n",
            "Epoch[13/15], Step [420/469], Reconst Loss: 10552.0713, KL Div: 3265.6946\n",
            "Epoch[13/15], Step [430/469], Reconst Loss: 10269.4551, KL Div: 3272.0386\n",
            "Epoch[13/15], Step [440/469], Reconst Loss: 10494.9805, KL Div: 3317.1379\n",
            "Epoch[13/15], Step [450/469], Reconst Loss: 10182.8291, KL Div: 3200.4746\n",
            "Epoch[13/15], Step [460/469], Reconst Loss: 9695.0518, KL Div: 3172.7573\n",
            "Epoch[14/15], Step [10/469], Reconst Loss: 10252.4941, KL Div: 3386.5479\n",
            "Epoch[14/15], Step [20/469], Reconst Loss: 9923.5312, KL Div: 3211.4392\n",
            "Epoch[14/15], Step [30/469], Reconst Loss: 10471.3926, KL Div: 3217.2205\n",
            "Epoch[14/15], Step [40/469], Reconst Loss: 10075.9863, KL Div: 3137.7734\n",
            "Epoch[14/15], Step [50/469], Reconst Loss: 10395.6807, KL Div: 3351.7139\n",
            "Epoch[14/15], Step [60/469], Reconst Loss: 10384.9238, KL Div: 3217.1826\n",
            "Epoch[14/15], Step [70/469], Reconst Loss: 10137.4805, KL Div: 3209.1929\n",
            "Epoch[14/15], Step [80/469], Reconst Loss: 9633.7324, KL Div: 3183.4126\n",
            "Epoch[14/15], Step [90/469], Reconst Loss: 10289.0879, KL Div: 3270.7156\n",
            "Epoch[14/15], Step [100/469], Reconst Loss: 10214.6914, KL Div: 3200.7402\n",
            "Epoch[14/15], Step [110/469], Reconst Loss: 9877.8672, KL Div: 3130.3782\n",
            "Epoch[14/15], Step [120/469], Reconst Loss: 9938.4609, KL Div: 3064.5830\n",
            "Epoch[14/15], Step [130/469], Reconst Loss: 10324.4844, KL Div: 3225.0308\n",
            "Epoch[14/15], Step [140/469], Reconst Loss: 10020.2520, KL Div: 3222.5891\n",
            "Epoch[14/15], Step [150/469], Reconst Loss: 10062.1172, KL Div: 3324.7212\n",
            "Epoch[14/15], Step [160/469], Reconst Loss: 9667.7344, KL Div: 3162.8665\n",
            "Epoch[14/15], Step [170/469], Reconst Loss: 9995.7754, KL Div: 3235.1758\n",
            "Epoch[14/15], Step [180/469], Reconst Loss: 10120.1973, KL Div: 3272.9375\n",
            "Epoch[14/15], Step [190/469], Reconst Loss: 10501.1836, KL Div: 3336.1191\n",
            "Epoch[14/15], Step [200/469], Reconst Loss: 10320.5518, KL Div: 3283.4565\n",
            "Epoch[14/15], Step [210/469], Reconst Loss: 10141.1807, KL Div: 3172.7622\n",
            "Epoch[14/15], Step [220/469], Reconst Loss: 10288.6895, KL Div: 3278.9756\n",
            "Epoch[14/15], Step [230/469], Reconst Loss: 10615.2402, KL Div: 3193.4014\n",
            "Epoch[14/15], Step [240/469], Reconst Loss: 10125.9414, KL Div: 3201.6997\n",
            "Epoch[14/15], Step [250/469], Reconst Loss: 10286.9570, KL Div: 3313.3892\n",
            "Epoch[14/15], Step [260/469], Reconst Loss: 10022.5273, KL Div: 3213.1899\n",
            "Epoch[14/15], Step [270/469], Reconst Loss: 10196.1621, KL Div: 3297.2344\n",
            "Epoch[14/15], Step [280/469], Reconst Loss: 9500.6211, KL Div: 3233.5322\n",
            "Epoch[14/15], Step [290/469], Reconst Loss: 10267.5449, KL Div: 3341.6357\n",
            "Epoch[14/15], Step [300/469], Reconst Loss: 10228.7803, KL Div: 3200.0974\n",
            "Epoch[14/15], Step [310/469], Reconst Loss: 9880.3281, KL Div: 3155.6255\n",
            "Epoch[14/15], Step [320/469], Reconst Loss: 10052.7139, KL Div: 3157.6680\n",
            "Epoch[14/15], Step [330/469], Reconst Loss: 9911.9590, KL Div: 3218.4714\n",
            "Epoch[14/15], Step [340/469], Reconst Loss: 10335.9385, KL Div: 3163.5317\n",
            "Epoch[14/15], Step [350/469], Reconst Loss: 10556.6523, KL Div: 3433.9614\n",
            "Epoch[14/15], Step [360/469], Reconst Loss: 10065.0039, KL Div: 3168.7363\n",
            "Epoch[14/15], Step [370/469], Reconst Loss: 10021.4629, KL Div: 3253.7705\n",
            "Epoch[14/15], Step [380/469], Reconst Loss: 10194.7568, KL Div: 3211.9268\n",
            "Epoch[14/15], Step [390/469], Reconst Loss: 10397.8008, KL Div: 3261.4509\n",
            "Epoch[14/15], Step [400/469], Reconst Loss: 10491.0820, KL Div: 3280.1282\n",
            "Epoch[14/15], Step [410/469], Reconst Loss: 10553.7314, KL Div: 3284.0737\n",
            "Epoch[14/15], Step [420/469], Reconst Loss: 10279.8633, KL Div: 3219.0237\n",
            "Epoch[14/15], Step [430/469], Reconst Loss: 10758.0332, KL Div: 3334.4265\n",
            "Epoch[14/15], Step [440/469], Reconst Loss: 10166.4238, KL Div: 3219.4736\n",
            "Epoch[14/15], Step [450/469], Reconst Loss: 10178.7559, KL Div: 3261.9722\n",
            "Epoch[14/15], Step [460/469], Reconst Loss: 10212.0977, KL Div: 3255.3740\n",
            "Epoch[15/15], Step [10/469], Reconst Loss: 9898.1279, KL Div: 3208.7727\n",
            "Epoch[15/15], Step [20/469], Reconst Loss: 9990.3545, KL Div: 3137.9297\n",
            "Epoch[15/15], Step [30/469], Reconst Loss: 9582.5723, KL Div: 3244.1638\n",
            "Epoch[15/15], Step [40/469], Reconst Loss: 10099.4883, KL Div: 3179.9106\n",
            "Epoch[15/15], Step [50/469], Reconst Loss: 9914.1357, KL Div: 3148.1387\n",
            "Epoch[15/15], Step [60/469], Reconst Loss: 9935.3193, KL Div: 3201.9910\n",
            "Epoch[15/15], Step [70/469], Reconst Loss: 10337.3027, KL Div: 3313.3643\n",
            "Epoch[15/15], Step [80/469], Reconst Loss: 10680.6455, KL Div: 3287.6948\n",
            "Epoch[15/15], Step [90/469], Reconst Loss: 10301.7988, KL Div: 3259.8481\n",
            "Epoch[15/15], Step [100/469], Reconst Loss: 10441.9561, KL Div: 3234.3638\n",
            "Epoch[15/15], Step [110/469], Reconst Loss: 10210.1191, KL Div: 3342.6985\n",
            "Epoch[15/15], Step [120/469], Reconst Loss: 10181.7734, KL Div: 3320.1653\n",
            "Epoch[15/15], Step [130/469], Reconst Loss: 9856.5547, KL Div: 3228.9141\n",
            "Epoch[15/15], Step [140/469], Reconst Loss: 10649.8301, KL Div: 3263.2583\n",
            "Epoch[15/15], Step [150/469], Reconst Loss: 10347.5645, KL Div: 3351.4180\n",
            "Epoch[15/15], Step [160/469], Reconst Loss: 10524.7070, KL Div: 3285.5818\n",
            "Epoch[15/15], Step [170/469], Reconst Loss: 10340.6016, KL Div: 3251.2751\n",
            "Epoch[15/15], Step [180/469], Reconst Loss: 10164.9688, KL Div: 3274.2283\n",
            "Epoch[15/15], Step [190/469], Reconst Loss: 9906.5322, KL Div: 3296.6270\n",
            "Epoch[15/15], Step [200/469], Reconst Loss: 10722.1133, KL Div: 3257.3647\n",
            "Epoch[15/15], Step [210/469], Reconst Loss: 9985.4707, KL Div: 3238.6074\n",
            "Epoch[15/15], Step [220/469], Reconst Loss: 10695.8623, KL Div: 3343.3684\n",
            "Epoch[15/15], Step [230/469], Reconst Loss: 10225.0332, KL Div: 3316.6802\n",
            "Epoch[15/15], Step [240/469], Reconst Loss: 10305.2607, KL Div: 3307.6560\n",
            "Epoch[15/15], Step [250/469], Reconst Loss: 9992.9414, KL Div: 3209.0449\n",
            "Epoch[15/15], Step [260/469], Reconst Loss: 9725.5605, KL Div: 3154.3711\n",
            "Epoch[15/15], Step [270/469], Reconst Loss: 10140.8662, KL Div: 3217.3860\n",
            "Epoch[15/15], Step [280/469], Reconst Loss: 9987.1357, KL Div: 3267.2878\n",
            "Epoch[15/15], Step [290/469], Reconst Loss: 10110.3125, KL Div: 3177.2661\n",
            "Epoch[15/15], Step [300/469], Reconst Loss: 9830.2021, KL Div: 3227.6841\n",
            "Epoch[15/15], Step [310/469], Reconst Loss: 10054.2051, KL Div: 3219.3438\n",
            "Epoch[15/15], Step [320/469], Reconst Loss: 10370.8896, KL Div: 3291.0171\n",
            "Epoch[15/15], Step [330/469], Reconst Loss: 10258.6152, KL Div: 3256.6230\n",
            "Epoch[15/15], Step [340/469], Reconst Loss: 10127.7275, KL Div: 3261.2949\n",
            "Epoch[15/15], Step [350/469], Reconst Loss: 10005.0000, KL Div: 3146.8149\n",
            "Epoch[15/15], Step [360/469], Reconst Loss: 10664.9844, KL Div: 3314.3726\n",
            "Epoch[15/15], Step [370/469], Reconst Loss: 9749.2471, KL Div: 3232.0620\n",
            "Epoch[15/15], Step [380/469], Reconst Loss: 10232.9082, KL Div: 3248.5977\n",
            "Epoch[15/15], Step [390/469], Reconst Loss: 9960.0391, KL Div: 3146.9902\n",
            "Epoch[15/15], Step [400/469], Reconst Loss: 9992.8770, KL Div: 3297.2148\n",
            "Epoch[15/15], Step [410/469], Reconst Loss: 9895.3594, KL Div: 3113.8965\n",
            "Epoch[15/15], Step [420/469], Reconst Loss: 9877.8223, KL Div: 3199.8389\n",
            "Epoch[15/15], Step [430/469], Reconst Loss: 10906.1094, KL Div: 3492.2510\n",
            "Epoch[15/15], Step [440/469], Reconst Loss: 10011.4160, KL Div: 3146.8267\n",
            "Epoch[15/15], Step [450/469], Reconst Loss: 9906.2930, KL Div: 3204.1250\n",
            "Epoch[15/15], Step [460/469], Reconst Loss: 10408.2285, KL Div: 3321.2163\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}